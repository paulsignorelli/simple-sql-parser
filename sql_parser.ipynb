{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e952b1f2-7762-4669-82f7-72f8034aeb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install sqlparse\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3be6b544-f5bc-41fe-88d7-ffdb4d59e07b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sqlparse\n",
    "\n",
    "def prettify_final(query_string: str):\n",
    "    # Format with sqlparse (keeps <<>> for any missing)\n",
    "    prettified_value = sqlparse.format(query_string, reindent=True, keyword_case='upper')\n",
    "    return prettified_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b270872b-b23e-4c3f-959b-182f8a8c77cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def strip_comments(sql: str) -> str:\n",
    "    # Remove multiline comments like /* ... */\n",
    "    sql = re.sub(r'/\\*.*?\\*/', '', sql, flags=re.DOTALL)\n",
    "\n",
    "    # Remove inline and full-line comments starting with --\n",
    "    sql = re.sub(r'--[^\\n\\r]*', '', sql)\n",
    "\n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03807546-abb9-4103-a926-d0ceff502123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_cte_block(sql_text: str):\n",
    "    \"\"\"\n",
    "    Extracts the full WITH clause including all CTEs and returns:\n",
    "    - the full CTE block (starting with WITH)\n",
    "    - the remainder of the query (starting after the last closing paren of the CTE block)\n",
    "    \"\"\"\n",
    "    sql_text = sql_text.strip()\n",
    "    if not sql_text[:4].upper() == \"WITH\":\n",
    "        return None, sql_text\n",
    "\n",
    "    depth = 0\n",
    "    i = 0\n",
    "    while i < len(sql_text):\n",
    "        if sql_text[i] == '(':\n",
    "            depth += 1\n",
    "        elif sql_text[i] == ')':\n",
    "            depth -= 1\n",
    "        elif sql_text[i:i+6].upper() == 'SELECT' and depth == 0 and i > 4:\n",
    "            # Found a top-level SELECT not inside any CTE block\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    # Keep scanning until we get out of the last CTE's final parenthesis\n",
    "    while i < len(sql_text) and depth > 0:\n",
    "        if sql_text[i] == '(':\n",
    "            depth += 1\n",
    "        elif sql_text[i] == ')':\n",
    "            depth -= 1\n",
    "        i += 1\n",
    "\n",
    "    # Move forward to the first SELECT after the CTE block\n",
    "    remaining_sql = sql_text[i:].lstrip()\n",
    "    select_match = re.match(r'(?is)^SELECT\\b', remaining_sql)\n",
    "    if select_match:\n",
    "        return sql_text[:i].strip(), remaining_sql\n",
    "    else:\n",
    "        # Could not find SELECT after WITH block, return full input\n",
    "        return sql_text, \"\"\n",
    "\n",
    "\n",
    "def extract_top_level_selects_with_joins(sql_text: str):\n",
    "    \"\"\"\n",
    "    Extracts:\n",
    "    - Top-level SELECT statements\n",
    "    - UNION, UNION ALL, INTERSECT, EXCEPT as separate sections\n",
    "    Ignores subqueries (SELECTs inside parentheses).\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    buffer = ''\n",
    "    depth = 0\n",
    "    i = 0\n",
    "    length = len(sql_text)\n",
    "\n",
    "    while i < length:\n",
    "        # Handle strings safely\n",
    "        if sql_text[i] in [\"'\", '\"']:\n",
    "            quote_char = sql_text[i]\n",
    "            buffer += quote_char\n",
    "            i += 1\n",
    "            while i < length and sql_text[i] != quote_char:\n",
    "                buffer += sql_text[i]\n",
    "                i += 1\n",
    "            if i < length:\n",
    "                buffer += quote_char\n",
    "                i += 1\n",
    "            continue\n",
    "\n",
    "        # Detect UNION keywords at depth 0\n",
    "        if depth == 0:\n",
    "            union_match = re.match(r'\\s*(UNION\\s+ALL|UNION|INTERSECT|EXCEPT)\\b', sql_text[i:], re.IGNORECASE)\n",
    "            if union_match:\n",
    "                if buffer.strip():\n",
    "                    result.append(buffer.strip())\n",
    "                result.append(union_match.group(1).upper())\n",
    "                i += union_match.end()\n",
    "                buffer = ''\n",
    "                continue\n",
    "\n",
    "            if sql_text[i:i+6].upper() == 'SELECT':\n",
    "                if buffer.strip():\n",
    "                    result.append(buffer.strip())\n",
    "                buffer = 'SELECT'\n",
    "                i += 6\n",
    "                continue\n",
    "\n",
    "        char = sql_text[i]\n",
    "        if char == '(':\n",
    "            depth += 1\n",
    "        elif char == ')':\n",
    "            depth -= 1\n",
    "        buffer += char\n",
    "        i += 1\n",
    "\n",
    "    if buffer.strip():\n",
    "        result.append(buffer.strip())\n",
    "\n",
    "    # Filter out any block that is a subquery like \"(SELECT ...\"\n",
    "    return [s for s in result if not s.lstrip().startswith('(')]\n",
    "\n",
    "def split_main_sections_with_classification(sql: str):\n",
    "    \"\"\"\n",
    "    Returns a list of dicts:\n",
    "    [\n",
    "        { \"type\": 'cte' | 'select' | 'join', \"id\": '1', \"content\": str },\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    sql = strip_comments(sql)\n",
    "    cte_block, remainder = extract_cte_block(sql)\n",
    "    sections = []\n",
    "    id_counter = 1\n",
    "\n",
    "    if cte_block:\n",
    "        sections.append({\n",
    "            \"type\": \"cte\",\n",
    "            \"section_id\": str(id_counter),\n",
    "            \"content\": cte_block\n",
    "        })\n",
    "        id_counter += 1\n",
    "\n",
    "    for part in extract_top_level_selects_with_joins(remainder):\n",
    "        stripped = part.strip().upper()\n",
    "        if stripped.startswith('SELECT'):\n",
    "            section_type = \"select\"\n",
    "        elif stripped in ('UNION', 'UNION ALL', 'INTERSECT', 'EXCEPT'):\n",
    "            section_type = \"join\"\n",
    "        else:\n",
    "            section_type = \"unknown\"\n",
    "\n",
    "        sections.append({\n",
    "            \"type\": section_type,\n",
    "            \"section_id\": str(id_counter),\n",
    "            \"content\": part\n",
    "        })\n",
    "        id_counter += 1\n",
    "\n",
    "    return sections\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f85b60f-7bec-4d5d-8d83-63b487e49e55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_and_replace_subqueries(sql_query, section:str=\"default\", section_id:int=0):\n",
    "    def parse(sql, base_idx=1):\n",
    "        subqueries = []\n",
    "        result = \"\"\n",
    "        i = 0\n",
    "        n = len(sql)\n",
    "        subquery_counter = base_idx\n",
    "\n",
    "        while i < n:\n",
    "            # Copy normal text\n",
    "            if sql[i] != '(':\n",
    "                result += sql[i]\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # We hit a '(', skip whitespace to see if SELECT\n",
    "            j = i + 1\n",
    "            while j < n and sql[j].isspace():\n",
    "                j += 1\n",
    "\n",
    "            if j + 5 <= n and sql[j:j+6].lower() == 'select':\n",
    "                # We found a (SELECT...\n",
    "                paren_count = 1\n",
    "                k = j + 6\n",
    "                buffer = '(' + sql[j:j+6]\n",
    "\n",
    "                while k < n and paren_count > 0:\n",
    "                    buffer += sql[k]\n",
    "                    if sql[k] == '(':\n",
    "                        paren_count += 1\n",
    "                    elif sql[k] == ')':\n",
    "                        paren_count -= 1\n",
    "                    k += 1\n",
    "\n",
    "                # Recursive replacement inside this subquery\n",
    "                inner_sql, inner_subs, next_counter = parse(buffer[1:-1], subquery_counter)\n",
    "                subqueries.extend(inner_subs)\n",
    "\n",
    "                # Store the current subquery properly as (placeholder, content)\n",
    "                placeholder = f\"<<subquery_{section}[{section_id}]_{next_counter}>>\"\n",
    "                subqueries.append((placeholder, inner_sql))  # <-- fix here\n",
    "                result += f\"({placeholder})\"\n",
    "                subquery_counter = next_counter + 1\n",
    "                i = k\n",
    "            else:\n",
    "                # Just a normal '('\n",
    "                result += sql[i]\n",
    "                i += 1\n",
    "\n",
    "        return result, subqueries, subquery_counter\n",
    "\n",
    "    clean_sql = strip_comments(sql_query)\n",
    "    pretty_sql = prettify_final(clean_sql)\n",
    "    # print(f\"pretty:\\n{pretty_sql}\\n\")\n",
    "    final_sql, all_subqueries, _ = parse(pretty_sql)\n",
    "    return final_sql, all_subqueries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5d30786-fe72-4e07-8af8-a8c1936be972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_sections_with_subquery_extraction(sql: str):\n",
    "    \"\"\"\n",
    "    Runs extract_and_replace_subqueries on each 'cte' and 'select' section.\n",
    "    Keeps 'join' sections unchanged.\n",
    "\n",
    "    Returns a list of dicts:\n",
    "    [\n",
    "        {\n",
    "            \"type\": \"cte\" | \"select\" | \"join\",\n",
    "            \"content\": \"modified content\",\n",
    "            \"subqueries\": [ (placeholder, original_subquery), ... ]\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    sections = split_main_sections_with_classification(sql)\n",
    "    processed = []\n",
    "\n",
    "    for section in sections:\n",
    "        if section[\"type\"] in (\"cte\", \"select\"):\n",
    "            replaced_sql, subqueries = extract_and_replace_subqueries(\n",
    "                section[\"content\"], \n",
    "                section=section[\"type\"],\n",
    "                section_id=section[\"id\"]\n",
    "            )\n",
    "            processed.append({\n",
    "                \"section_id\": section[\"id\"],\n",
    "                \"type\": section[\"type\"],\n",
    "                \"content\": replaced_sql,\n",
    "                \"subqueries\": subqueries\n",
    "            })\n",
    "        else:  # e.g., join, unknown\n",
    "            processed.append({\n",
    "                \"section_id\": section[\"id\"],\n",
    "                \"type\": section[\"type\"],\n",
    "                \"content\": section[\"content\"],\n",
    "                \"subqueries\": []\n",
    "            })\n",
    "\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce6c4e42-e424-4f29-8fe5-e297a23ef20c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType\n",
    "\n",
    "# Initialize SparkSession if needed\n",
    "spark = SparkSession.builder.appName(\"SQLSubqueries\").getOrCreate()\n",
    "\n",
    "def get_schema():\n",
    "    schema = StructType([\n",
    "        StructField(\"section\", StringType(), True),\n",
    "        StructField(\"section_id\", IntegerType(), True),\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"identifier\", StringType(), True),\n",
    "        StructField(\"original_query\", StringType(), True),\n",
    "        StructField(\"final_query\", StringType(), True),\n",
    "        StructField(\"content\", StringType(), True),\n",
    "        StructField(\"columns\", ArrayType(StringType()), True),\n",
    "        StructField(\"converted_content\", StringType(), True),\n",
    "        StructField(\"converted_columns\", ArrayType(StringType()), True),\n",
    "        StructField(\"response_error\", StringType(), True)\n",
    "    ])\n",
    "    return schema\n",
    "\n",
    "def initialize_empty_subquery_delta_table(table_name):\n",
    "    \"\"\"\n",
    "    Creates or overwrites a Delta table with the expected schema but no data.\n",
    "    \"\"\"\n",
    "\n",
    "    if table_name is None or len(table_name.strip()) == 0 or table_name == \"\":\n",
    "        return\n",
    "    \n",
    "    # Create empty DataFrame\n",
    "    empty_df = spark.createDataFrame([], get_schema())\n",
    "\n",
    "    # Overwrite the Delta table\n",
    "    spark.sql(f\"drop table if exists {table_name}\")\n",
    "    empty_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "  \n",
    "def subqueries_to_spark_dataframe(sql_query: str):\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    sections = split_main_sections_with_classification(sql_query)\n",
    "    # sections = process_sections_with_subquery_extraction(sql_query)\n",
    "    all_records = []\n",
    "\n",
    "    for idx, section in enumerate(sections):\n",
    "        modified_sections = []\n",
    "        section_type = section[\"type\"]\n",
    "        section_id = int(section[\"section_id\"])\n",
    "        \n",
    "        if section_type in (\"cte\", \"select\"):\n",
    "            modified_sql, subqueries = extract_and_replace_subqueries(section[\"content\"], section=section_type, section_id=section_id)\n",
    "            modified_sections.append(modified_sql)\n",
    "            print(f\"{section['content']}\")\n",
    "\n",
    "            for ident, content in subqueries:\n",
    "                print(f\"appending {content}\")\n",
    "                all_records.append({\n",
    "                    \"section\": section_type,\n",
    "                    \"section_id\": section_id,\n",
    "                    \"type\": \"subquery\",\n",
    "                    \"identifier\": ident.replace(\"<<\", \"\").replace(\">>\", \"\"),\n",
    "                    \"original_query\": None,\n",
    "                    \"final_query\": \"\",\n",
    "                    \"content\": content,\n",
    "                    \"columns\": [],\n",
    "                    \"converted_content\": \"\",\n",
    "                    \"converted_columns\": [],\n",
    "                    \"response_error\": \"\"\n",
    "                })\n",
    "        else:\n",
    "            # join sections\n",
    "            all_records.append({\n",
    "                    \"section\": section_type,\n",
    "                    \"section_id\": section_id,\n",
    "                    \"type\": \"\",\n",
    "                    \"identifier\": \"\",\n",
    "                    \"original_query\": None,\n",
    "                    \"final_query\": \"\",\n",
    "                    \"content\": section[\"content\"],\n",
    "                    \"columns\": [],\n",
    "                    \"converted_content\": \"\",\n",
    "                    \"converted_columns\": [],\n",
    "                    \"response_error\": \"\"\n",
    "                })\n",
    "            modified_sections.append(section[\"content\"])\n",
    "\n",
    "        final_query = \"\\n\".join(modified_sections)\n",
    "        # Add the top-level reconstructed query\n",
    "        \n",
    "        all_records.append({\n",
    "            \"section\": section_type,\n",
    "            \"section_id\": section_id,\n",
    "            \"type\": \"section_main\",\n",
    "            \"identifier\": f\"section_{section_id}_main\",\n",
    "            \"original_query\": section[\"content\"],\n",
    "            \"final_query\": \"\",\n",
    "            \"content\": final_query,\n",
    "            \"columns\": [],\n",
    "            \"converted_content\": \"\",\n",
    "            \"converted_columns\": [],\n",
    "            \"response_error\": \"\"\n",
    "        })\n",
    "\n",
    "        # # Add the top-level reconstructed query\n",
    "        # all_records.append({\n",
    "        #     \"section\": \"main\",\n",
    "        #     \"section_id\": 0,\n",
    "        #     \"type\": \"main\",\n",
    "        #     \"identifier\": \"main\",\n",
    "        #     \"original_query\": sql_query,\n",
    "        #     \"final_query\": \"\",\n",
    "        #     \"content\": final_query,\n",
    "        #     \"columns\": [],\n",
    "        #     \"converted_content\": \"\",\n",
    "        #     \"converted_columns\": [],\n",
    "        #     \"response_error\": \"\"\n",
    "        # })\n",
    "\n",
    "    schema = get_schema()\n",
    "    return spark.createDataFrame(all_records, schema)\n",
    "\n",
    "def write_subqueries_to_delta(df, table_name: str):\n",
    "    \"\"\"\n",
    "    Writes the Spark DataFrame of subqueries and main query\n",
    "    to a Delta table in overwrite mode.\n",
    "    \"\"\"\n",
    "    if table_name is None or len(table_name.strip()) == 0 or table_name == \"\":\n",
    "        return\n",
    "    df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7e476bb-2a6a-4da9-840d-0483e62044c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "\n",
    "def extract_columns_and_replace_select(df):\n",
    "    \"\"\"\n",
    "    Takes a Spark DataFrame of queries, extracts columns from each SELECT,\n",
    "    and replaces SELECT list in every query (subquery and main) with '*'.\n",
    "    Returns a new DataFrame with an added `columns` array field.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    def get_columns_and_rewrite(query):\n",
    "        # crude regex to get columns between SELECT and FROM\n",
    "        pattern = re.compile(r\"(select)(.*?)(from)\", re.IGNORECASE | re.DOTALL)\n",
    "        match = pattern.search(query)\n",
    "        if match:\n",
    "            cols_section = match.group(2)\n",
    "            parts = re.split(r\",(?![^(]*\\))\", cols_section)\n",
    "            columns = [col.strip() for col in parts if col.strip()]\n",
    "            # replace SELECT columns with SELECT *\n",
    "            modified_query = pattern.sub(r\"\\1 * \\3\", query)\n",
    "            return columns, modified_query\n",
    "        else:\n",
    "            return [], query\n",
    "\n",
    "    rows = df.collect()\n",
    "    records = []\n",
    "\n",
    "    for row in rows:\n",
    "        columns, modified_content = get_columns_and_rewrite(row['content'])\n",
    "        records.append({\n",
    "            \"section\": row['section'],\n",
    "            \"section_id\": row[\"section_id\"],\n",
    "            \"type\": row['type'],\n",
    "            \"identifier\": row['identifier'],\n",
    "            \"original_query\": row['original_query'],\n",
    "            \"final_query\": \"\",\n",
    "            \"content\": modified_content,\n",
    "            \"columns\": columns,\n",
    "            \"converted_content\": \"\",\n",
    "            \"converted_columns\": [],\n",
    "            \"response_error\": \"\"            \n",
    "        })\n",
    "\n",
    "    schema = get_schema()\n",
    "    return spark.createDataFrame(records, schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12de1cc2-2684-42b3-ab96-91486f6db9b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import ChatMessage, ChatMessageRole\n",
    "import traceback\n",
    "\n",
    "def convert_query_to_databricks_sql(query: str, endpoint_name: str = \"databricks-claude-sonnet-4\"):\n",
    "    if endpoint_name is None or len(endpoint_name.strip()) == 0:\n",
    "        print(\"\\n✅ LLM endpoint disabled\\n\")\n",
    "        print(f\"converted qry = {query}\")\n",
    "        return query\n",
    "    w = WorkspaceClient()  # Initialize without timeout parameter, set timeout if supported later\n",
    "    response = w.serving_endpoints.query(\n",
    "        # name=\"databricks-claude-3-7-sonnet\",\n",
    "        name=endpoint_name,\n",
    "        # name=\"llama-70b-code-converstion\",\n",
    "        messages=[\n",
    "            ChatMessage(\n",
    "                role=ChatMessageRole.SYSTEM, content=\"You are a helpful assistant.\"\n",
    "            ),\n",
    "            ChatMessage(\n",
    "                role=ChatMessageRole.USER, content=f\"Please covert the following Oracle SQL query to Databricks SQL. Just return the query, no other content, including ```sql. If you see any sql that is wrapped in << >>, for example <<subquery_1>>, assume it is valid sql and leave it as is.  I need a complete conversion, do not skip any lines:\\n{query}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    response = response.choices[0].message.content.strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f85778bd-e65e-42ff-a4a9-68639555e740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convert_sql(df, endpoint_name=\"databricks-claude-sonnet-4\"):\n",
    "    \"\"\"\n",
    "    Takes a Spark DataFrame of queries, loops through each row,\n",
    "    calls convert_query_to_databricks_sql to convert it,\n",
    "    and returns a new Spark DataFrame with an added converted_content field.\n",
    "    \"\"\"\n",
    "    rows = df.collect()\n",
    "    records = []\n",
    "\n",
    "    for row in rows:\n",
    "        if row['section'] == \"join\":\n",
    "            # Row(section='join', section_id=3, type='', identifier='', original_query=None, final_query='', content='UNION ALL', columns=[], converted_content='', converted_columns=[], response_error='')\n",
    "            records.append({\n",
    "                \"section\": row['section'],\n",
    "                \"section_id\": row[\"section_id\"],\n",
    "                \"type\": row[\"type\"],\n",
    "                \"identifier\": row[\"identifier\"],\n",
    "                \"original_query\": row[\"original_query\"],\n",
    "                \"final_query\": row[\"final_query\"],\n",
    "                \"content\": row['content'],\n",
    "                \"columns\": [],\n",
    "                \"converted_content\": \"\",\n",
    "                \"converted_columns\": [],\n",
    "                \"response_error\": \"\"\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # call your existing function for conversion\n",
    "        # llm_response = convert_query_to_databricks_sql(row['content'], endpoint_name=endpoint_name)\n",
    "        # converted_sql = llm_response.choices[0].message.content.strip()\n",
    "        response_error = \"\"\n",
    "        try:\n",
    "            converted_sql = convert_query_to_databricks_sql(row['content'], endpoint_name=endpoint_name)\n",
    "        except Exception as e:\n",
    "            converted_sql = \"\"\n",
    "            response_error = f\"{type(e).__name__}: {str(e)}\\n{traceback.format_exc(limit=2)}\"\n",
    "\n",
    "        records.append({\n",
    "            \"section\": row['section'],\n",
    "            \"section_id\": row[\"section_id\"],\n",
    "            \"type\": row['type'],\n",
    "            \"identifier\": row['identifier'],\n",
    "            \"original_query\": row['original_query'],\n",
    "            \"final_query\": \"\",\n",
    "            \"content\": row['content'],\n",
    "            \"columns\": row['columns'],\n",
    "            \"converted_content\": converted_sql,\n",
    "            \"converted_columns\": [],\n",
    "            \"response_error\": response_error\n",
    "        })\n",
    "\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "    schema = get_schema()\n",
    "\n",
    "    return spark.createDataFrame(records, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d27696fc-8f16-47ad-98d2-f7e90ea8aa6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convert_sql_on_columns(df, chunk_size=10, endpoint_name=\"databricks-claude-sonnet-4\"):\n",
    "    \"\"\"\n",
    "    Loops through the dataframe, chunks the columns, calls convert_query_to_databricks_sql\n",
    "    on a dummy SELECT, and aggregates converted columns into `converted_columns`.\n",
    "    Returns a new Spark DataFrame.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    rows = df.collect()\n",
    "    records = []\n",
    "\n",
    "    for row in rows:\n",
    "        if row['section'] == \"join\":\n",
    "            # build the new record\n",
    "            records.append({\n",
    "                \"section\": row['section'],\n",
    "                \"section_id\": row[\"section_id\"],\n",
    "                \"type\": row[\"type\"],\n",
    "                \"identifier\": row[\"identifier\"],\n",
    "                \"original_query\": row[\"original_query\"],\n",
    "                \"final_query\": row[\"final_query\"],\n",
    "                \"content\": row['content'],\n",
    "                \"columns\": [],\n",
    "                \"converted_content\": \"\",\n",
    "                \"converted_columns\": [],\n",
    "                \"response_error\": \"\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        converted_columns = []\n",
    "\n",
    "        if row['columns']:\n",
    "            # break columns into chunks\n",
    "            col_chunks = [row['columns'][i:i+chunk_size] for i in range(0, len(row['columns']), chunk_size)]\n",
    "            print(f\"col_chunks:\\n{col_chunks}\")\n",
    "\n",
    "            for chunk in col_chunks:\n",
    "                dummy_query = f\"SELECT {', '.join(chunk)} FROM dummy\"\n",
    "                print(f\"dummy_query = {dummy_query}\")\n",
    "\n",
    "                # call your existing function exactly as you wrote it\n",
    "                # response = convert_query_to_databricks_sql(dummy_query, endpoint_name)\n",
    "                # converted_sql = response.choices[0].message.content.strip()\n",
    "                response_error = \"\"\n",
    "                try:\n",
    "                    print(f\"endpoint_name = {endpoint_name}\")\n",
    "                    converted_sql = convert_query_to_databricks_sql(dummy_query, endpoint_name=endpoint_name)\n",
    "                except Exception as e:\n",
    "                    converted_sql = \"\"\n",
    "                    response_error = f\"{type(e).__name__}: {str(e)}\\n{traceback.format_exc(limit=2)}\"\n",
    "                    continue\n",
    "                # extract columns between SELECT and FROM\n",
    "                match = re.search(r\"(?is)select\\s+(.*?)(?:\\s+from\\b|$)\", converted_sql)\n",
    "                print(f\"converted_sql = {converted_sql}\")\n",
    "                if match:\n",
    "                    cols_section = match.group(1)\n",
    "                    print(f\"cols_section = {cols_section}\")\n",
    "                    cols = [c.strip() for c in re.split(r\",(?![^(]*\\))\", cols_section) if c.strip()]\n",
    "                    converted_columns.extend(cols)\n",
    "\n",
    "        # build the new record\n",
    "        records.append({\n",
    "            \"section\": row['section'],\n",
    "            \"section_id\": row[\"section_id\"],\n",
    "            \"type\": row['type'],\n",
    "            \"identifier\": row['identifier'],\n",
    "            \"original_query\": row['original_query'],\n",
    "            \"final_query\": \"\",\n",
    "            \"content\": row['content'],\n",
    "            \"columns\": row['columns'],\n",
    "            \"converted_content\": row['converted_content'],\n",
    "            \"converted_columns\": converted_columns,\n",
    "            \"response_error\": response_error\n",
    "        })\n",
    "\n",
    "    return spark.createDataFrame(records, get_schema())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3f49716-0a39-4204-b848-18dbbf374c68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def assemble_final_query_string(converted_df):\n",
    "#     \"\"\"\n",
    "#     - Replaces SELECT * with converted columns if present.\n",
    "#     - Recursively replaces <<subquery_x>> placeholders across all rows.\n",
    "#     - Returns final fully assembled main query string.\n",
    "#     \"\"\"\n",
    "\n",
    "#     import re\n",
    "#     rows = converted_df.collect()\n",
    "#     query_map = {}\n",
    "\n",
    "#     # Build initial map with SELECT * replaced by converted columns if present\n",
    "#     for row in rows:\n",
    "#         name = row['identifier']\n",
    "#         query_text = row['converted_content'] if row['converted_content'] else row['content']\n",
    "\n",
    "#         if row['columns'] and row['converted_columns']:\n",
    "#             all_converted_cols = \", \".join(row['converted_columns'])\n",
    "#             query_text = re.sub(\n",
    "#                 r'(?i)(select\\s+)\\*',\n",
    "#                 lambda m: m.group(1) + all_converted_cols,\n",
    "#                 query_text,\n",
    "#                 count=1\n",
    "#             )\n",
    "\n",
    "#         query_map[name] = query_text\n",
    "\n",
    "#     # Recursive placeholder replacement until stable\n",
    "#     changed = True\n",
    "#     while changed:\n",
    "#         changed = False\n",
    "#         for name, text in query_map.items():\n",
    "#             original_text = text\n",
    "#             for sub_name, sub_text in query_map.items():\n",
    "#                 if sub_name != name:\n",
    "#                     text = text.replace(f\"<<{sub_name}>>\", sub_text)\n",
    "#             if text != original_text:\n",
    "#                 changed = True\n",
    "#             query_map[name] = text\n",
    "\n",
    "#     # Return the final resolved main query\n",
    "#     return query_map.get('main', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5a36c1e-9d2a-44ae-baf7-b8c26e3d558c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def assemble_final_query_string(df):\n",
    "    \"\"\"\n",
    "    - Replaces SELECT * with converted columns if available.\n",
    "    - Resolves subquery placeholders within each section_main.\n",
    "    - Assembles the full final query using ordered section_main parts and join parts.\n",
    "    \"\"\"\n",
    "    rows = df.collect()\n",
    "    query_map = {}\n",
    "\n",
    "    # Step 1: Build map of identifier -> resolved content\n",
    "    for row in rows:\n",
    "        identifier = row['identifier']\n",
    "        content = row['converted_content'] if row['converted_content'] else row['content']\n",
    "        if not isinstance(content, str):\n",
    "            content = \"\"\n",
    "\n",
    "        if row['columns'] and row['converted_columns']:\n",
    "            try:\n",
    "                columns = ast.literal_eval(row['columns']) if isinstance(row['columns'], str) else row['columns']\n",
    "                converted_columns = ast.literal_eval(row['converted_columns']) if isinstance(row['converted_columns'], str) else row['converted_columns']\n",
    "                if columns and converted_columns:\n",
    "                    converted_cols_str = \", \".join(converted_columns)\n",
    "                    content = re.sub(r'(?i)(select\\s+)\\*', lambda m: m.group(1) + converted_cols_str, content, count=1)\n",
    "            except Exception as e:\n",
    "                # print(f\"e = {e}\")\n",
    "                pass\n",
    "\n",
    "        query_map[identifier] = content\n",
    "\n",
    "    # print(f\"query_map = {query_map}\")\n",
    "\n",
    "    # Step 2: Resolve all <<subquery_x>> placeholders recursively\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        for name, text in query_map.items():\n",
    "            original_text = text\n",
    "            for sub_name, sub_text in query_map.items():\n",
    "                print(f\"sub_name = {sub_name}, sub_text = {sub_text}\")\n",
    "                if sub_name != name and isinstance(sub_text, str):\n",
    "                    text = text.replace(f\"<<{sub_name}>>\", sub_text)\n",
    "            if text != original_text:\n",
    "                changed = True\n",
    "            query_map[name] = text\n",
    "\n",
    "    # print(f\"query_map_relplaced = {query_map}\")\n",
    "\n",
    "    # Step 3: Assemble all section_main and join entries in order\n",
    "    df_main = df.filter(df[\"type\"].isin([\"section_main\", \"join\"]))\n",
    "    df_main = df_main.withColumn(\"section_id\", df_main[\"section_id\"].cast(\"int\"))\n",
    "    ordered_rows = df_main.sort(\"section_id\").collect()\n",
    "\n",
    "    # print(f\"ordered_rows = {ordered_rows}\")\n",
    "    \n",
    "    final_parts = []\n",
    "    for row in ordered_rows:\n",
    "        identifier = row['identifier']\n",
    "        resolved = query_map.get(identifier, row['converted_content'] or row['content'])\n",
    "        final_parts.append(resolved.strip() if isinstance(resolved, str) else \"\")\n",
    "\n",
    "    return \"\\n\\n\".join(final_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0390417a-91a7-4db9-8b75-191eb9e22005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def append_final_query_row(df, final_query_str):\n",
    "    \"\"\"\n",
    "    Appends a new row with identifier 'final_query' and the provided final query string\n",
    "    into the 'final_query' column. Other fields set to None or empty.\n",
    "    Assumes DataFrame has the schema with all expected columns.\n",
    "    \"\"\"\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"section\", StringType(), True),\n",
    "        StructField(\"section_id\", IntegerType(), True),\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"identifier\", StringType(), True),\n",
    "        StructField(\"original_query\", StringType(), True),\n",
    "        StructField(\"final_query\", StringType(), True),\n",
    "        StructField(\"content\", StringType(), True),\n",
    "        StructField(\"columns\", ArrayType(StringType()), True),\n",
    "        StructField(\"converted_content\", StringType(), True),\n",
    "        StructField(\"converted_columns\", ArrayType(StringType()), True),\n",
    "        StructField(\"response_error\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    # build single new row\n",
    "    new_row_data = [{\n",
    "        \"section\": \"final_query\",\n",
    "        \"section_id\": 0,\n",
    "        \"type\": \"final\",\n",
    "        \"identifier\": \"final_query\",\n",
    "        \"original_query\": None,\n",
    "        \"final_query\": final_query_str,\n",
    "        \"content\": None,\n",
    "        \"columns\": [],\n",
    "        \"converted_content\": None,\n",
    "        \"converted_columns\": [],\n",
    "        \"response_error\": None\n",
    "    }]\n",
    "    new_row_df = spark.createDataFrame(new_row_data, schema)\n",
    "\n",
    "    # union with existing DataFrame\n",
    "    final_df = df.unionByName(new_row_df)\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed1b5f90-e4d3-410c-95d3-aedd01347f3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# def generate_complex_query():\n",
    "#     base_query = \"\"\"\n",
    "# WITH\n",
    "# DeptStats AS (\n",
    "#     SELECT\n",
    "#         DepartmentID,\n",
    "#         SUM(Salary) AS TotalDeptSalary,\n",
    "#         COUNT(*) AS NumEmployees,\n",
    "#         AVG(Salary) AS AvgDeptSalary\n",
    "#     FROM\n",
    "#         Employees\n",
    "#     GROUP BY\n",
    "#         DepartmentID\n",
    "# ),\n",
    "# EmpProjects AS (\n",
    "#     SELECT\n",
    "#         EmployeeID,\n",
    "#         COUNT(ProjectID) AS ProjectsCompleted,\n",
    "#         MAX(CompletedDate) AS LastProjectDate,\n",
    "#         YEAR(MAX(CompletedDate)) AS LastProjectYear\n",
    "#     FROM\n",
    "#         Projects\n",
    "#     WHERE\n",
    "#         Status = 'Completed'\n",
    "#     GROUP BY\n",
    "#         EmployeeID\n",
    "# )\n",
    "\n",
    "# SELECT\n",
    "#     e.EmployeeID,\n",
    "#     UPPER(e.Name) AS Name,\n",
    "#     d.Name AS Department,\n",
    "#     CASE\n",
    "#         WHEN e.Salary > (\n",
    "#             SELECT AVG(Salary)\n",
    "#             FROM Employees\n",
    "#             WHERE DepartmentID = e.DepartmentID\n",
    "#         ) THEN CONCAT('Above Average (', CAST(e.Salary AS VARCHAR), ')')\n",
    "#         ELSE 'Average or Below'\n",
    "#     END AS SalaryStatus,\n",
    "#     -- some remark here,\n",
    "#     CASE\n",
    "#         WHEN    rsm.investment_type = 'BL'\n",
    "#             AND NVL (psah.acrd_cd, 'N') NOT IN ('Y', 'V') -- story 897300\n",
    "#         THEN\n",
    "#             NVL (\n",
    "#                 (SELECT wacoupon\n",
    "#                    FROM stg_wso_pos_acr_ame\n",
    "#                   WHERE     portfolio_fund_id = psah.cal_dt\n",
    "#                         AND asofdate = psah.cal_dt\n",
    "#                         AND asset_primaryud = psah.asset_id\n",
    "#                         AND rec_typ_cd = 'POS'),\n",
    "#                 0)\n",
    "#         ELSE\n",
    "#             psah.int_rt\n",
    "#     END\n",
    "#         AS pos_int_it,  \n",
    "#     ep.ProjectsCompleted,\n",
    "#     YEAR(e.HireDate) AS HireYear,\n",
    "#     MONTH(e.HireDate) AS HireMonth,\n",
    "#     COALESCE(ep.LastProjectYear, 'N/A') AS LastProjectYear\n",
    "# FROM\n",
    "#     Employees e\n",
    "#     JOIN Departments d ON e.DepartmentID = d.DepartmentID\n",
    "#     LEFT JOIN EmpProjects ep ON e.EmployeeID = ep.EmployeeID\n",
    "# WHERE\n",
    "#     e.EmployeeID IN (\n",
    "#         SELECT\n",
    "#             e2.EmployeeID\n",
    "#         FROM\n",
    "#             Employees e2\n",
    "#             JOIN Departments d2 ON e2.DepartmentID = d2.DepartmentID\n",
    "#             LEFT JOIN EmpProjects ep2 ON e2.EmployeeID = ep2.EmployeeID\n",
    "#         WHERE\n",
    "#             e2.Salary > (\n",
    "#                 SELECT AVG(Salary)\n",
    "#                 FROM Employees\n",
    "#                 WHERE DepartmentID = e2.DepartmentID\n",
    "#             )\n",
    "#     )\n",
    "# UNION ALL\n",
    "# SELECT\n",
    "#     NULL AS EmployeeID,\n",
    "#     NULL AS Name,\n",
    "#     d.Name AS Department,\n",
    "#     CONCAT('Department Total: ', CAST(ds.TotalDeptSalary AS VARCHAR)) AS SalaryStatus,\n",
    "#     ds.NumEmployees AS ProjectsCompleted,\n",
    "#     NULL AS HireYear,\n",
    "#     NULL AS HireMonth,\n",
    "#     NULL AS LastProjectYear,\n",
    "# \"\"\"\n",
    "\n",
    "#     # Generate 599 columns, each randomly a literal or a CASE with subquery\n",
    "#     cols = []\n",
    "#     for i in range(1, 10):\n",
    "#         rnd = random.random()\n",
    "#         if i == 1:\n",
    "#             col = (\n",
    "#                 f\"CASE WHEN ds.TotalDeptSalary > 100000 AND ds.TotalDeptSalary < 200000 AND NVL(a.emp_class, 'N') NOT IN ('A', 'B') -- story 1234\\n\"\n",
    "#                 f\"THEN NVL((SELECT COUNT(*) FROM Employees WHERE DepartmentID = ds.DepartmentID),0)\\n\"\n",
    "#                 f\"ELSE 0 END as col{i}\"\n",
    "#             )\n",
    "#         else:\n",
    "#             col = f\"'col{i}' as col{i}\"\n",
    "#         cols.append(col)\n",
    "#     cols_str = \",\\n\".join(cols)\n",
    "\n",
    "#     # Assemble the final query\n",
    "#     final_query = f\"\"\"\n",
    "# {base_query}\n",
    "# {cols_str}\n",
    "# FROM\n",
    "#     DeptStats ds\n",
    "#     JOIN Departments d ON ds.DepartmentID = d.DepartmentID;\n",
    "# \"\"\"\n",
    "#     return final_query\n",
    "\n",
    "# query_string = generate_complex_query()\n",
    "# print(query_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "750bae44-3511-41bb-9a6a-6ba266c57809",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# parse_log_table=\"users.paul_signorelli.sql_parsing_log\"\n",
    "# endpoint_name=\"\"\n",
    "# initialize_empty_subquery_delta_table(table_name=parse_log_table)\n",
    "# spark_df = subqueries_to_spark_dataframe(query_string)\n",
    "# spark_df_with_columns = extract_columns_and_replace_select(spark_df)\n",
    "# spark_df_converted = convert_sql(spark_df_with_columns, endpoint_name=endpoint_name)\n",
    "# spark_df_with_converted_columns = convert_sql_on_columns(spark_df_converted, chunk_size=5, endpoint_name=endpoint_name)\n",
    "\n",
    "# final_query = assemble_final_query_string(spark_df_with_converted_columns)\n",
    "# pretty_final = prettify_final(final_query)\n",
    "\n",
    "# df_final = append_final_query_row(spark_df_with_converted_columns, pretty_final)\n",
    "# write_subqueries_to_delta(df_final, table_name=parse_log_table)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8e56e93-df60-40e8-a209-cdbdb6312d28",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"columns\":337},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752598877230}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(\n",
    "#   spark.sql(f\"select * from {parse_log_table}\")\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_c75093c8-0895-475e-8c1b-6acacfe3368b",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1938256853172048,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "sql_parser",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
