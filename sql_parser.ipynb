{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c9748f7-7219-4822-81a7-6446e7cb948d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Simple Query Parser\n",
    "\n",
    "This parser will deconstuct a complex query into its sub-queries so each of those sub-queries can be separated out to be analyzed or converted.\n",
    "\n",
    "It will look for SELECT, cte's defined by WITH, IN or EXISTS to discover sub-queries.\n",
    "\n",
    "It will also can remove the columns if need be if a query has a long list of columns and replace them with a '*'.  This can be done to reduce query size for an LLM and analyze the columns separately.\n",
    "\n",
    "The script will put all of this information into a collection which can be iterated through to call an LLM to analyze or migrate the code and then it can be pieced back together at the end.\n",
    "\n",
    "So the following query:\n",
    "\n",
    "```\n",
    "with cte1 as (\n",
    "        select cust_id, sum(sales) as sales_sum \n",
    "        from orders\n",
    "        group by cust_id\n",
    "    ),\n",
    "    cte2 as (\n",
    "        select cust_id, sum(expenses) as expenses\n",
    "        from expenses\n",
    "        group by cust_id\n",
    "    )\n",
    "    select cust_id, sales_sum, expenses\n",
    "    from cte1\n",
    "    inner join cte2 on cte1.cust_id = cte2.cust_id\n",
    "    where cte1.cust_id in (select cust_id from customer_region where region = 'USA')\n",
    "    and exists (select 1 from sales_person_region where region = 'NYC')\n",
    "```\n",
    "\n",
    "will be deconstructed to look like\n",
    "\n",
    "```\n",
    "\n",
    "Query 1 (cte1):\n",
    "    select * \n",
    "        from orders\n",
    "        group by cust_id\n",
    "Columns:\n",
    "  - cust_id\n",
    "  - sum(sales) as sales_sum\n",
    "----------------------------------------\n",
    "Query 2 (cte2):\n",
    "    select *\n",
    "        from expenses\n",
    "        group by cust_id\n",
    "Columns:\n",
    "  - cust_id\n",
    "  - sum(expenses) as expenses\n",
    "----------------------------------------\n",
    "Query 3 (subquery):\n",
    "    select * from customer_region where region = 'USA'\n",
    "Columns:\n",
    "  - cust_id\n",
    "----------------------------------------\n",
    "Query 4 (subquery):\n",
    "    select * from sales_person_region where region = 'NYC'\n",
    "Columns:\n",
    "  - 1\n",
    "----------------------------------------\n",
    "Query 5 (main):\n",
    "    select *\n",
    "    from cte1\n",
    "    inner join cte2 on cte1.cust_id = cte2.cust_id\n",
    "    where cte1.cust_id in (<<query 2>>)\n",
    "    and exists (<<query 3>>)\n",
    "Columns:\n",
    "  - cust_id\n",
    "  - sales_sum\n",
    "  - expenses\n",
    "----------------------------------------\n",
    "main query:\n",
    "with\n",
    "    cte1 as (\n",
    "        <<query 1>>\n",
    "    ),\n",
    "    cte2 as (\n",
    "        <<query 2>>\n",
    "    )\n",
    "   <<query 5>>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e952b1f2-7762-4669-82f7-72f8034aeb0d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Libraries"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow-skinny[databricks] langgraph==0.3.4 databricks-langchain databricks-agents uv sqlparse\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "590e7431-3e6b-4549-abb7-88c0c8975f59",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Agent"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "module_path = os.path.join(current_dir, \".\")  # replace with actual folder\n",
    "sys.path.insert(0, module_path)\n",
    "from agent import AGENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3be6b544-f5bc-41fe-88d7-ffdb4d59e07b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Prettify SQL"
    }
   },
   "outputs": [],
   "source": [
    "import sqlparse\n",
    "\n",
    "def prettify_final(query_string: str):\n",
    "    # Format with sqlparse (keeps <<>> for any missing)\n",
    "    prettified_value = sqlparse.format(query_string, reindent=True, keyword_case='upper')\n",
    "    return prettified_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b270872b-b23e-4c3f-959b-182f8a8c77cc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Strip Comments from SQL"
    }
   },
   "outputs": [],
   "source": [
    "def strip_comments(sql: str) -> str:\n",
    "    # Remove multiline comments like /* ... */\n",
    "    sql = re.sub(r'/\\*.*?\\*/', '', sql, flags=re.DOTALL)\n",
    "\n",
    "    # Remove inline and full-line comments starting with --\n",
    "    sql = re.sub(r'--[^\\n\\r]*', '', sql)\n",
    "\n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5924e58-a7f1-4121-8a5b-b6e8e04ea7e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_tables(content: str):\n",
    "\n",
    "  # with open(sql_file, 'r') as f:\n",
    "  #   content = f.read()\n",
    "  \n",
    "  # Remove comments\n",
    "  content = re.sub(r'--.*$', '', content, flags=re.MULTILINE)\n",
    "  content = re.sub(r'/\\*.*?\\*/', '', content, flags=re.DOTALL)\n",
    "  # Find tables after FROM/JOIN\n",
    "  pattern = r'\\b(?:FROM|JOIN)\\s+([a-zA-Z_][a-zA-Z0-9_.]*)'\n",
    "  tables = re.findall(pattern, content, re.IGNORECASE)\n",
    "  # Clean up common SQL keywords\n",
    "  keywords = {'ON', 'WHERE', 'GROUP', 'ORDER', 'HAVING', 'SELECT', 'INNER', 'LEFT', 'RIGHT', 'OUTER'}\n",
    "  tables = [t for t in tables if t.upper() not in keywords]\n",
    "  return sorted(set(tables))\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#   if len(sys.argv) < 2:\n",
    "#     print(\"Usage: python script.py <sql_file>\")\n",
    "#     sys.exit(1)\n",
    "#   sql_file = sys.argv[1] # This is like $1 in bash\n",
    "#   try:\n",
    "#     tables = extract_tables(sql_file)\n",
    "#     for table in tables:\n",
    "#       print(table)\n",
    "#   except FileNotFoundError:\n",
    "#     print(f\"Error: File '{sql_file}' not found\")\n",
    "#     sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03807546-abb9-4103-a926-d0ceff502123",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Extract Main Sections"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_cte_block(sql_text: str):\n",
    "    \"\"\"\n",
    "    Extracts the full WITH clause including all CTEs and returns:\n",
    "    - the full CTE block (starting with WITH)\n",
    "    - the remainder of the query (starting after the last closing paren of the CTE block)\n",
    "    \"\"\"\n",
    "    sql_text = sql_text.strip()\n",
    "    if not sql_text[:4].upper() == \"WITH\":\n",
    "        return None, sql_text\n",
    "\n",
    "    depth = 0\n",
    "    i = 0\n",
    "    while i < len(sql_text):\n",
    "        if sql_text[i] == '(':\n",
    "            depth += 1\n",
    "        elif sql_text[i] == ')':\n",
    "            depth -= 1\n",
    "        elif sql_text[i:i+6].upper() == 'SELECT' and depth == 0 and i > 4:\n",
    "            # Found a top-level SELECT not inside any CTE block\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    # Keep scanning until we get out of the last CTE's final parenthesis\n",
    "    while i < len(sql_text) and depth > 0:\n",
    "        if sql_text[i] == '(':\n",
    "            depth += 1\n",
    "        elif sql_text[i] == ')':\n",
    "            depth -= 1\n",
    "        i += 1\n",
    "\n",
    "    # Move forward to the first SELECT after the CTE block\n",
    "    remaining_sql = sql_text[i:].lstrip()\n",
    "    select_match = re.match(r'(?is)^SELECT\\b', remaining_sql)\n",
    "    if select_match:\n",
    "        return sql_text[:i].strip(), remaining_sql\n",
    "    else:\n",
    "        # Could not find SELECT after WITH block, return full input\n",
    "        return sql_text, \"\"\n",
    "\n",
    "\n",
    "def extract_top_level_selects_with_joins(sql_text: str):\n",
    "    \"\"\"\n",
    "    Extracts:\n",
    "    - Top-level SELECT statements\n",
    "    - UNION, UNION ALL, INTERSECT, EXCEPT as separate sections\n",
    "    Ignores subqueries (SELECTs inside parentheses).\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    buffer = ''\n",
    "    depth = 0\n",
    "    i = 0\n",
    "    length = len(sql_text)\n",
    "\n",
    "    while i < length:\n",
    "        # Handle strings safely\n",
    "        if sql_text[i] in [\"'\", '\"']:\n",
    "            quote_char = sql_text[i]\n",
    "            buffer += quote_char\n",
    "            i += 1\n",
    "            while i < length and sql_text[i] != quote_char:\n",
    "                buffer += sql_text[i]\n",
    "                i += 1\n",
    "            if i < length:\n",
    "                buffer += quote_char\n",
    "                i += 1\n",
    "            continue\n",
    "\n",
    "        # Detect UNION keywords at depth 0\n",
    "        if depth == 0:\n",
    "            union_match = re.match(r'\\s*(UNION\\s+ALL|UNION|INTERSECT|EXCEPT)\\b', sql_text[i:], re.IGNORECASE)\n",
    "            if union_match:\n",
    "                if buffer.strip():\n",
    "                    result.append(buffer.strip())\n",
    "                result.append(union_match.group(1).upper())\n",
    "                i += union_match.end()\n",
    "                buffer = ''\n",
    "                continue\n",
    "\n",
    "            if sql_text[i:i+6].upper() == 'SELECT':\n",
    "                if buffer.strip():\n",
    "                    result.append(buffer.strip())\n",
    "                buffer = 'SELECT'\n",
    "                i += 6\n",
    "                continue\n",
    "\n",
    "        char = sql_text[i]\n",
    "        if char == '(':\n",
    "            depth += 1\n",
    "        elif char == ')':\n",
    "            depth -= 1\n",
    "        buffer += char\n",
    "        i += 1\n",
    "\n",
    "    if buffer.strip():\n",
    "        result.append(buffer.strip())\n",
    "\n",
    "    # Filter out any block that is a subquery like \"(SELECT ...\"\n",
    "    return [s for s in result if not s.lstrip().startswith('(')]\n",
    "\n",
    "def split_main_sections_with_classification(sql: str):\n",
    "    \"\"\"\n",
    "    Returns a list of dicts:\n",
    "    [\n",
    "        { \"type\": 'cte' | 'select' | 'join', \"id\": '1', \"content\": str },\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    sql = strip_comments(sql)\n",
    "    cte_block, remainder = extract_cte_block(sql)\n",
    "    sections = []\n",
    "    id_counter = 1\n",
    "\n",
    "    if cte_block:\n",
    "        sections.append({\n",
    "            \"type\": \"cte\",\n",
    "            \"section_id\": str(id_counter),\n",
    "            \"content\": cte_block\n",
    "        })\n",
    "        id_counter += 1\n",
    "\n",
    "    for part in extract_top_level_selects_with_joins(remainder):\n",
    "        stripped = part.strip().upper()\n",
    "        if stripped.startswith('SELECT'):\n",
    "            section_type = \"select\"\n",
    "        elif stripped in ('UNION', 'UNION ALL', 'INTERSECT', 'EXCEPT'):\n",
    "            section_type = \"join\"\n",
    "        else:\n",
    "            section_type = \"unknown\"\n",
    "\n",
    "        sections.append({\n",
    "            \"type\": section_type,\n",
    "            \"section_id\": str(id_counter),\n",
    "            \"content\": part\n",
    "        })\n",
    "        id_counter += 1\n",
    "\n",
    "    return sections\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f85b60f-7bec-4d5d-8d83-63b487e49e55",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Extract Subqueries from All Main Sections"
    }
   },
   "outputs": [],
   "source": [
    "# def extract_and_replace_subqueries(sql_query, section:str=\"default\", section_id:int=0):\n",
    "#     def parse(sql, base_idx=1):\n",
    "#         subqueries = []\n",
    "#         result = \"\"\n",
    "#         i = 0\n",
    "#         n = len(sql)\n",
    "#         subquery_counter = base_idx\n",
    "\n",
    "#         while i < n:\n",
    "#             # Copy normal text\n",
    "#             if sql[i] != '(':\n",
    "#                 result += sql[i]\n",
    "#                 i += 1\n",
    "#                 continue\n",
    "\n",
    "#             # We hit a '(', skip whitespace to see if SELECT\n",
    "#             j = i + 1\n",
    "#             while j < n and sql[j].isspace():\n",
    "#                 j += 1\n",
    "\n",
    "#             if j + 5 <= n and sql[j:j+6].lower() == 'select':\n",
    "#                 # We found a (SELECT...\n",
    "#                 paren_count = 1\n",
    "#                 k = j + 6\n",
    "#                 buffer = '(' + sql[j:j+6]\n",
    "\n",
    "#                 while k < n and paren_count > 0:\n",
    "#                     buffer += sql[k]\n",
    "#                     if sql[k] == '(':\n",
    "#                         paren_count += 1\n",
    "#                     elif sql[k] == ')':\n",
    "#                         paren_count -= 1\n",
    "#                     k += 1\n",
    "\n",
    "#                 # Recursive replacement inside this subquery\n",
    "#                 inner_sql, inner_subs, next_counter = parse(buffer[1:-1], subquery_counter)\n",
    "#                 subqueries.extend(inner_subs)\n",
    "\n",
    "#                 # Store the current subquery properly as (placeholder, content)\n",
    "#                 placeholder = f\"<<subquery_{section}[{section_id}]_{next_counter}>>\"\n",
    "#                 subqueries.append((placeholder, inner_sql))  # <-- fix here\n",
    "#                 result += f\"({placeholder})\"\n",
    "#                 subquery_counter = next_counter + 1\n",
    "#                 i = k\n",
    "#             else:\n",
    "#                 # Just a normal '('\n",
    "#                 result += sql[i]\n",
    "#                 i += 1\n",
    "\n",
    "#         return result, subqueries, subquery_counter\n",
    "\n",
    "#     clean_sql = strip_comments(sql_query)\n",
    "#     pretty_sql = prettify_final(clean_sql)\n",
    "#     # print(f\"pretty:\\n{pretty_sql}\\n\")\n",
    "#     final_sql, all_subqueries, _ = parse(pretty_sql)\n",
    "#     return final_sql, all_subqueries\n",
    "\n",
    "def extract_and_replace_subqueries(\n",
    "    sql_query,\n",
    "    section: str = \"default\",\n",
    "    section_id: int = 0,\n",
    "    scalar_funcs=None,\n",
    "    scalar_operators=None\n",
    "):\n",
    "    if scalar_funcs is None:\n",
    "        scalar_funcs = {\"NVL\", \"COALESCE\", \"ROUND\", \"ABS\", \"LTRIM\", \"RTRIM\", \"UPPER\", \"LOWER\"}\n",
    "\n",
    "    if scalar_operators is None:\n",
    "        scalar_operators = {\"=\", \">\", \"<\", \">=\", \"<=\", \"<>\", \"!=\", \"+\", \"-\", \"*\", \"/\"}\n",
    "\n",
    "    def parse(sql, base_idx=1):\n",
    "        subqueries = []\n",
    "        result = \"\"\n",
    "        i = 0\n",
    "        n = len(sql)\n",
    "        subquery_counter = base_idx\n",
    "\n",
    "        while i < n:\n",
    "            if sql[i] != '(':\n",
    "                result += sql[i]\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            j = i + 1\n",
    "            while j < n and sql[j].isspace():\n",
    "                j += 1\n",
    "\n",
    "            if j + 5 <= n and sql[j:j+6].lower() == 'select':\n",
    "                # ----- Improved scalar detection -----\n",
    "                preceding_text = sql[:i].upper()\n",
    "                # Find last 20 characters before '(' for function name\n",
    "                lookback = preceding_text[max(0, i-200):i]\n",
    "                \n",
    "                # Match functions like NVL(...) before the subquery\n",
    "                func_match = re.search(r'([A-Z_]+)\\s*\\(\\s*$', lookback)\n",
    "                found_func = func_match.group(1) if func_match else \"\"\n",
    "                is_scalar = found_func in scalar_funcs or \\\n",
    "                            any(op in lookback for op in scalar_operators)\n",
    "\n",
    "                # -------------------------------------\n",
    "\n",
    "                paren_count = 1\n",
    "                k = j + 6\n",
    "                buffer = '(' + sql[j:j+6]\n",
    "                while k < n and paren_count > 0:\n",
    "                    buffer += sql[k]\n",
    "                    if sql[k] == '(':\n",
    "                        paren_count += 1\n",
    "                    elif sql[k] == ')':\n",
    "                        paren_count -= 1\n",
    "                    k += 1\n",
    "\n",
    "                inner_sql, inner_subs, next_counter = parse(buffer[1:-1], subquery_counter)\n",
    "                subqueries.extend(inner_subs)\n",
    "\n",
    "                placeholder = f\"<<subquery_{section}[{section_id}]_{next_counter}>>\"\n",
    "                subqueries.append((placeholder, inner_sql, is_scalar))\n",
    "                result += f\"({placeholder})\"\n",
    "                subquery_counter = next_counter + 1\n",
    "                i = k\n",
    "            else:\n",
    "                result += sql[i]\n",
    "                i += 1\n",
    "\n",
    "        return result, subqueries, subquery_counter\n",
    "\n",
    "    clean_sql = strip_comments(sql_query)\n",
    "    pretty_sql = prettify_final(clean_sql)\n",
    "    final_sql, all_subqueries, _ = parse(pretty_sql)\n",
    "    print(all_subqueries)\n",
    "    return final_sql, all_subqueries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce6c4e42-e424-4f29-8fe5-e297a23ef20c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Extract Subqueries to Dataframe"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType, BooleanType\n",
    "\n",
    "# Initialize SparkSession if needed\n",
    "spark = SparkSession.builder.appName(\"SQLSubqueries\").getOrCreate()\n",
    "\n",
    "def get_schema():\n",
    "    schema = StructType([\n",
    "        StructField(\"section\", StringType(), True),\n",
    "        StructField(\"section_id\", IntegerType(), True),\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"identifier\", StringType(), True),\n",
    "        StructField(\"is_scalar\", BooleanType(), True),\n",
    "        StructField(\"original_query\", StringType(), True),\n",
    "        StructField(\"final_query\", StringType(), True),\n",
    "        StructField(\"content\", StringType(), True),\n",
    "        StructField(\"columns\", ArrayType(StringType()), True),\n",
    "        StructField(\"tables\", ArrayType(StringType()), True),\n",
    "        StructField(\"functions\", ArrayType(StringType()), True),\n",
    "        StructField(\"converted_content\", StringType(), True),\n",
    "        StructField(\"converted_columns\", ArrayType(StringType()), True),\n",
    "        StructField(\"response_error\", StringType(), True)\n",
    "    ])\n",
    "    return schema\n",
    "\n",
    "def initialize_empty_subquery_delta_table(table_name):\n",
    "    \"\"\"\n",
    "    Creates or overwrites a Delta table with the expected schema but no data.\n",
    "    \"\"\"\n",
    "\n",
    "    if table_name is None or len(table_name.strip()) == 0 or table_name == \"\":\n",
    "        return\n",
    "    \n",
    "    # Create empty DataFrame\n",
    "    empty_df = spark.createDataFrame([], get_schema())\n",
    "\n",
    "    # Overwrite the Delta table\n",
    "    spark.sql(f\"drop table if exists {table_name}\")\n",
    "    empty_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "  \n",
    "def subqueries_to_spark_dataframe(sql_query: str):\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    sections = split_main_sections_with_classification(sql_query)\n",
    "    # sections = process_sections_with_subquery_extraction(sql_query)\n",
    "    all_records = []\n",
    "\n",
    "    for idx, section in enumerate(sections):\n",
    "        modified_sections = []\n",
    "        section_type = section[\"type\"]\n",
    "        section_id = int(section[\"section_id\"])\n",
    "        \n",
    "        if section_type in (\"cte\", \"select\"):\n",
    "            modified_sql, subqueries = extract_and_replace_subqueries(section[\"content\"], section=section_type, section_id=section_id)\n",
    "            modified_sections.append(modified_sql)\n",
    "\n",
    "            for ident, content, is_scalar in subqueries:\n",
    "                all_records.append({\n",
    "                    \"section\": section_type,\n",
    "                    \"section_id\": section_id,\n",
    "                    \"type\": \"subquery\",\n",
    "                    \"identifier\": ident.replace(\"<<\", \"\").replace(\">>\", \"\"),\n",
    "                    \"is_scalar\": is_scalar,\n",
    "                    \"original_query\": None,\n",
    "                    \"final_query\": \"\",\n",
    "                    \"content\": content,\n",
    "                    \"columns\": [],\n",
    "                    \"tables\": [],\n",
    "                    \"functions\": [],\n",
    "                    \"converted_content\": \"\",\n",
    "                    \"converted_columns\": [],\n",
    "                    \"response_error\": \"\"\n",
    "                })\n",
    "        else:\n",
    "            # join sections\n",
    "            all_records.append({\n",
    "                    \"section\": section_type,\n",
    "                    \"section_id\": section_id,\n",
    "                    \"type\": \"\",\n",
    "                    \"identifier\": \"\",\n",
    "                    \"is_scalar\": False,\n",
    "                    \"original_query\": None,\n",
    "                    \"final_query\": \"\",\n",
    "                    \"content\": section[\"content\"],\n",
    "                    \"columns\": [],\n",
    "                    \"tables\": [],\n",
    "                    \"functions\": [],\n",
    "                    \"converted_content\": \"\",\n",
    "                    \"converted_columns\": [],\n",
    "                    \"response_error\": \"\"\n",
    "                })\n",
    "            modified_sections.append(section[\"content\"])\n",
    "\n",
    "        final_query = \"\\n\".join(modified_sections)\n",
    "        # final_query_tables = extract_tables(final_query)\n",
    "\n",
    "        # Add the top-level reconstructed query\n",
    "        \n",
    "        all_records.append({\n",
    "            \"section\": section_type,\n",
    "            \"section_id\": section_id,\n",
    "            \"type\": \"section_main\",\n",
    "            \"identifier\": f\"section_{section_id}_main\",\n",
    "            \"is_scalar\": False,\n",
    "            \"original_query\": section[\"content\"],\n",
    "            \"final_query\": \"\",\n",
    "            \"content\": final_query,\n",
    "            \"columns\": [],\n",
    "            \"tables\": [],\n",
    "            \"functions\": [],\n",
    "            \"converted_content\": \"\",\n",
    "            \"converted_columns\": [],\n",
    "            \"response_error\": \"\"\n",
    "        })\n",
    "\n",
    "    schema = get_schema()\n",
    "    return spark.createDataFrame(all_records, schema)\n",
    "\n",
    "def write_subqueries_to_delta(df, table_name: str):\n",
    "    \"\"\"\n",
    "    Writes the Spark DataFrame of subqueries and main query\n",
    "    to a Delta table in overwrite mode.\n",
    "    \"\"\"\n",
    "    if table_name is None or len(table_name.strip()) == 0 or table_name == \"\":\n",
    "        return\n",
    "    df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7e476bb-2a6a-4da9-840d-0483e62044c3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Extract Columns to Dataframe"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "\n",
    "def extract_columns_and_replace_select(df):\n",
    "    \"\"\"\n",
    "    Takes a Spark DataFrame of queries, extracts top-level SELECT columns (parentheses-aware),\n",
    "    replaces the SELECT list with '*', and returns a new Spark DataFrame.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    def get_columns_and_rewrite(query):\n",
    "        query = query.strip()\n",
    "        upper_query = query.upper()\n",
    "\n",
    "        select_pos = upper_query.find(\"SELECT\")\n",
    "        if select_pos == -1:\n",
    "            return [], query\n",
    "\n",
    "        i = select_pos + 6\n",
    "        depth = 0\n",
    "        buffer = \"\"\n",
    "        columns = []\n",
    "        n = len(query)\n",
    "\n",
    "        while i < n:\n",
    "            if query[i] in (\"'\", '\"'):\n",
    "                quote_char = query[i]\n",
    "                buffer += quote_char\n",
    "                i += 1\n",
    "                while i < n and query[i] != quote_char:\n",
    "                    buffer += query[i]\n",
    "                    i += 1\n",
    "                if i < n:\n",
    "                    buffer += quote_char\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if query[i] == '(':\n",
    "                depth += 1\n",
    "            elif query[i] == ')':\n",
    "                depth -= 1\n",
    "            elif depth == 0 and query[i:i+5].upper() == 'FROM ':\n",
    "                columns.append(buffer.strip())\n",
    "                break\n",
    "            elif depth == 0 and query[i] == ',':\n",
    "                columns.append(buffer.strip())\n",
    "                buffer = \"\"\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            buffer += query[i]\n",
    "            i += 1\n",
    "\n",
    "        # Reconstruct query with SELECT * instead\n",
    "        prefix = query[:select_pos]\n",
    "        suffix = query[i:] if i < n else \"\"\n",
    "        modified_query = f\"{prefix}SELECT * {suffix}\".strip()\n",
    "\n",
    "        return [c for c in columns if c], modified_query\n",
    "\n",
    "    # Process rows\n",
    "    rows = df.collect()\n",
    "    records = []\n",
    "\n",
    "    for row in rows:\n",
    "        columns, modified_content = get_columns_and_rewrite(row['content'])\n",
    "\n",
    "        records.append({\n",
    "            \"section\": row['section'],\n",
    "            \"section_id\": row[\"section_id\"],\n",
    "            \"type\": row['type'],\n",
    "            \"identifier\": row['identifier'],\n",
    "            \"is_scalar\": row['is_scalar'],\n",
    "            \"original_query\": row['original_query'],\n",
    "            \"final_query\": \"\",\n",
    "            \"content\": modified_content,\n",
    "            \"columns\": columns,\n",
    "            \"tables\": row['tables'],\n",
    "            \"functions\": row['functions'],\n",
    "            \"converted_content\": \"\",\n",
    "            \"converted_columns\": [],\n",
    "            \"response_error\": \"\"            \n",
    "        })\n",
    "\n",
    "    schema = get_schema()\n",
    "    return spark.createDataFrame(records, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5ebd1a4-4785-4332-99c2-41a7cad9beb8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Find Matching Subqueries"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, collect_list, size, concat_ws\n",
    "\n",
    "def extract_tables_full(sql):\n",
    "    if not sql:\n",
    "        return []\n",
    "    from_tables = re.findall(r'FROM\\s+([^\\s;()]+)', sql, re.IGNORECASE)\n",
    "    join_tables = re.findall(r'JOIN\\s+([^\\s;()]+)', sql, re.IGNORECASE)\n",
    "    all_tables = from_tables + join_tables\n",
    "    return sorted(set(t.strip().lower() for t in all_tables))\n",
    "\n",
    "extract_tables_udf = udf(extract_tables_full, ArrayType(StringType()))\n",
    "\n",
    "# Main function\n",
    "def find_duplicate_subqueries_by_table_spark(df, content_col: str):\n",
    "    subqueries_df = df.filter(col(\"type\") == \"subquery\")\n",
    "\n",
    "    with_tables = subqueries_df.withColumn(\n",
    "        \"tables\", extract_tables_udf(col(content_col))\n",
    "    )\n",
    "\n",
    "    with_keys = with_tables.withColumn(\"table_key\", concat_ws(\",\", col(\"tables\")))\n",
    "\n",
    "    grouped = with_keys.groupBy(\"table_key\").agg(\n",
    "        collect_list(\"identifier\").alias(\"identifiers\")\n",
    "    ).filter(size(\"identifiers\") > 1)\n",
    "\n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c9f664-bc86-4b21-8f31-6b2a8eafac82",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Agent call"
    }
   },
   "outputs": [],
   "source": [
    "def query_to_databricks_sql_agent(query: str, is_scalar:bool = False):\n",
    "  query_message = f\"convert the following query: {query}\" if not is_scalar else f\"convert the following so that it only returns one record: {query}\"\n",
    "  response = AGENT.predict({\"messages\": [{\"role\": \"user\", \"content\": query_message}]})\n",
    "  for msg in response.messages:\n",
    "      if msg.role == \"assistant\":\n",
    "          return msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12de1cc2-2684-42b3-ab96-91486f6db9b8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Functiojn to Convert SQL with LLM"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import ChatMessage, ChatMessageRole\n",
    "import traceback\n",
    "\n",
    "def convert_query_to_databricks_sql(query: str, endpoint_name: str = \"databricks-claude-sonnet-4\"):\n",
    "    if endpoint_name is None or len(endpoint_name.strip()) == 0:\n",
    "        return query\n",
    "    w = WorkspaceClient()\n",
    "    response = w.serving_endpoints.query(\n",
    "        name=endpoint_name,\n",
    "        messages=[\n",
    "            ChatMessage(\n",
    "                role=ChatMessageRole.SYSTEM, content=\"You are a helpful assistant.\"\n",
    "            ),\n",
    "            ChatMessage(\n",
    "                role=ChatMessageRole.USER, content=f\"Please covert the following Oracle SQL query to Databricks SQL. Just return the query, no other content, including sql. If you see any sql that is wrapped in << >>, for example <<subquery_1>>, assume it is valid sql and leave it as is.  I need a complete conversion, do not skip any lines:\\n{query}\"\n",
    "            ),\n",
    "        ],\n",
    "        temperature=0,\n",
    "        top_p=0.95,\n",
    "        top_k=1,\n",
    "        max_tokens=None,\n",
    "        thinking=False\n",
    "    )\n",
    "    response = response.choices[0].message.content.strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2146065-834e-46aa-9d46-07cdb57262b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Pre-existing Log File"
    }
   },
   "outputs": [],
   "source": [
    "def load_parse_log(table_name: str):\n",
    "    \"\"\"\n",
    "    Loads a Spark DataFrame from a given table name.\n",
    "    \n",
    "    Parameters:\n",
    "        table_name (str): The fully qualified table name (e.g., 'db.schema.table' or 'default.my_table').\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The loaded Spark DataFrame.\n",
    "    \"\"\"\n",
    "    return spark.read.table(table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f85778bd-e65e-42ff-a4a9-68639555e740",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Conversion of Main SQL"
    }
   },
   "outputs": [],
   "source": [
    "def convert_sql(df, endpoint_name=\"databricks-claude-sonnet-4\", full_refresh=True):\n",
    "    \"\"\"\n",
    "    Takes a Spark DataFrame of queries, loops through each row,\n",
    "    calls convert_query_to_databricks_sql to convert it,\n",
    "    and returns a new Spark DataFrame with an added converted_content field.\n",
    "    \"\"\"\n",
    "    rows = df.collect()\n",
    "    records = []\n",
    "\n",
    "    for row in rows:\n",
    "        if row['section'] == \"join\":\n",
    "            # Row(section='join', section_id=3, type='', identifier='', original_query=None, final_query='', content='UNION ALL', columns=[], converted_content='', converted_columns=[], response_error='')\n",
    "            records.append({\n",
    "                \"section\": row['section'],\n",
    "                \"section_id\": row[\"section_id\"],\n",
    "                \"type\": row[\"type\"],\n",
    "                \"identifier\": row[\"identifier\"],\n",
    "                \"is_scalar\": row[\"is_scalar\"],\n",
    "                \"original_query\": row[\"original_query\"],\n",
    "                \"final_query\": row[\"final_query\"],\n",
    "                \"content\": row['content'],\n",
    "                \"columns\": [],\n",
    "                \"tables\": row['tables'],\n",
    "                \"functions\": row['functions'],\n",
    "                \"converted_content\": \"\",\n",
    "                \"converted_columns\": [],\n",
    "                \"response_error\": \"\"\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Skip conversion if converted_content is a non-empty string\n",
    "        # print(f\"{row['converted_content']}\")\n",
    "        if not full_refresh and len(str(row['converted_content']).strip()) > 0:\n",
    "            records.append(row.asDict())\n",
    "            # print(\"skip\")\n",
    "            continue\n",
    "\n",
    "        # print(\"convert\")\n",
    "\n",
    "        # call your existing function for conversion\n",
    "        # llm_response = convert_query_to_databricks_sql(row['content'], endpoint_name=endpoint_name)\n",
    "        # converted_sql = llm_response.choices[0].message.content.strip()\n",
    "        response_error = \"\"\n",
    "        try:\n",
    "            # converted_sql = convert_query_to_databricks_sql(row['content'], endpoint_name=endpoint_name)\n",
    "            converted_sql = query_to_databricks_sql_agent(row['content'], is_scalar=row['is_scalar'])\n",
    "        except Exception as e:\n",
    "            converted_sql = \"\"\n",
    "            response_error = f\"{type(e).__name__}: {str(e)}\\n{traceback.format_exc(limit=2)}\"\n",
    "\n",
    "        records.append({\n",
    "            \"section\": row['section'],\n",
    "            \"section_id\": row[\"section_id\"],\n",
    "            \"type\": row['type'],\n",
    "            \"identifier\": row['identifier'],\n",
    "            \"is_scalar\": row['is_scalar'],\n",
    "            \"original_query\": row['original_query'],\n",
    "            \"final_query\": \"\",\n",
    "            \"content\": row['content'],\n",
    "            \"columns\": row['columns'],\n",
    "            \"tables\": row['tables'],\n",
    "            \"functions\": row['functions'],\n",
    "            \"converted_content\": converted_sql,\n",
    "            \"converted_columns\": [],\n",
    "            \"response_error\": response_error\n",
    "        })\n",
    "\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "    schema = get_schema()\n",
    "\n",
    "    return spark.createDataFrame(records, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d27696fc-8f16-47ad-98d2-f7e90ea8aa6f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Conversion of Columns"
    }
   },
   "outputs": [],
   "source": [
    "def convert_sql_on_columns(df, chunk_size=10, endpoint_name=\"databricks-claude-sonnet-4\", full_refresh=True):\n",
    "    \"\"\"\n",
    "    Loops through the dataframe, chunks the columns, calls convert_query_to_databricks_sql\n",
    "    on a dummy SELECT, and aggregates converted columns into `converted_columns`.\n",
    "    Returns a new Spark DataFrame.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    rows = df.collect()\n",
    "    records = []\n",
    "\n",
    "    for row in rows:\n",
    "        if row['section'] == \"join\":\n",
    "            # build the new record\n",
    "            records.append({\n",
    "                \"section\": row['section'],\n",
    "                \"section_id\": row[\"section_id\"],\n",
    "                \"type\": row[\"type\"],\n",
    "                \"identifier\": row[\"identifier\"],\n",
    "                \"is_scalar\": row[\"is_scalar\"],\n",
    "                \"original_query\": row[\"original_query\"],\n",
    "                \"final_query\": row[\"final_query\"],\n",
    "                \"content\": row['content'],\n",
    "                \"columns\": [],\n",
    "                \"tables\": row['tables'],\n",
    "                \"functions\": row['functions'],\n",
    "                \"converted_content\": \"\",\n",
    "                \"converted_columns\": [],\n",
    "                \"response_error\": \"\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Skip conversion if converted_columns is a non-empty list\n",
    "        if not full_refresh and len(row['converted_columns']) > 0:\n",
    "            records.append(row.asDict())\n",
    "            # print(\"skip\")\n",
    "            continue\n",
    "\n",
    "        # print(\"convert\")\n",
    "\n",
    "        converted_columns = []\n",
    "        response_error = \"\"\n",
    "\n",
    "        if row['columns']:\n",
    "            # break columns into chunks\n",
    "            col_chunks = [row['columns'][i:i+chunk_size] for i in range(0, len(row['columns']), chunk_size)]\n",
    "            # print(f\"col_chunks:\\n{col_chunks}\")\n",
    "\n",
    "            for chunk in col_chunks:\n",
    "                dummy_query = f\"SELECT {', '.join(chunk)} FROM dummy\"\n",
    "                # print(f\"dummy_query = {dummy_query}\")\n",
    "\n",
    "                # call your existing function exactly as you wrote it\n",
    "                # response = convert_query_to_databricks_sql(dummy_query, endpoint_name)\n",
    "                # converted_sql = response.choices[0].message.content.strip()\n",
    "                response_error = \"\"\n",
    "                try:\n",
    "                    # print(f\"endpoint_name = {endpoint_name}\")\n",
    "                    # converted_sql = convert_query_to_databricks_sql(dummy_query, endpoint_name=endpoint_name)\n",
    "                    converted_sql = query_to_databricks_sql_agent(dummy_query, is_scalar=False)\n",
    "                except Exception as e:\n",
    "                    converted_sql = \"\"\n",
    "                    response_error = f\"{type(e).__name__}: {str(e)}\\n{traceback.format_exc(limit=2)}\"\n",
    "                    continue\n",
    "                # extract columns between SELECT and FROM\n",
    "                match = re.search(r\"(?is)select\\s+(.*?)(?:\\s+from\\b|$)\", converted_sql)\n",
    "                # print(f\"converted_sql = {converted_sql}\")\n",
    "                if match:\n",
    "                    cols_section = match.group(1)\n",
    "                    # print(f\"cols_section = {cols_section}\")\n",
    "                    cols = [c.strip() for c in re.split(r\",(?![^(]*\\))\", cols_section) if c.strip()]\n",
    "                    converted_columns.extend(cols)\n",
    "\n",
    "        # build the new record\n",
    "        records.append({\n",
    "            \"section\": row['section'],\n",
    "            \"section_id\": row[\"section_id\"],\n",
    "            \"type\": row['type'],\n",
    "            \"identifier\": row['identifier'],\n",
    "            \"is_scalar\": row['is_scalar'],\n",
    "            \"original_query\": row['original_query'],\n",
    "            \"final_query\": \"\",\n",
    "            \"content\": row['content'],\n",
    "            \"columns\": row['columns'],\n",
    "            \"tables\": row['tables'],\n",
    "            \"functions\": row['functions'],\n",
    "            \"converted_content\": row['converted_content'],\n",
    "            \"converted_columns\": converted_columns,\n",
    "            \"response_error\": response_error\n",
    "        })\n",
    "\n",
    "    return spark.createDataFrame(records, get_schema())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5a36c1e-9d2a-44ae-baf7-b8c26e3d558c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Assemble Final Query String"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def assemble_final_query_string(df):\n",
    "    \"\"\"\n",
    "    - Replaces SELECT * with converted columns if available.\n",
    "    - Resolves subquery placeholders within each section_main.\n",
    "    - Assembles the full final query using ordered section_main parts and join parts.\n",
    "    \"\"\"\n",
    "    rows = df.collect()\n",
    "    query_map = {}\n",
    "\n",
    "    # Step 1: Build map of identifier -> resolved content\n",
    "    for row in rows:\n",
    "        identifier = row['identifier']\n",
    "        content = row['converted_content'] if row['converted_content'] else row['content']\n",
    "        if not isinstance(content, str):\n",
    "            content = \"\"\n",
    "\n",
    "        if row['columns'] and row['converted_columns']:\n",
    "            try:\n",
    "                columns = ast.literal_eval(row['columns']) if isinstance(row['columns'], str) else row['columns']\n",
    "                converted_columns = ast.literal_eval(row['converted_columns']) if isinstance(row['converted_columns'], str) else row['converted_columns']\n",
    "                if columns and converted_columns:\n",
    "                    converted_cols_str = \", \".join(converted_columns)\n",
    "                    content = re.sub(r'(?i)(select\\s+)\\*', lambda m: m.group(1) + converted_cols_str, content, count=1)\n",
    "            except Exception as e:\n",
    "                # print(f\"e = {e}\")\n",
    "                pass\n",
    "\n",
    "        query_map[identifier] = content\n",
    "\n",
    "    # print(f\"query_map = {query_map}\")\n",
    "\n",
    "    # Step 2: Resolve all <<subquery_x>> placeholders recursively\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        for name, text in query_map.items():\n",
    "            original_text = text\n",
    "            for sub_name, sub_text in query_map.items():\n",
    "                # print(f\"sub_name = {sub_name}, sub_text = {sub_text}\")\n",
    "                if sub_name != name and isinstance(sub_text, str):\n",
    "                    text = text.replace(f\"<<{sub_name}>>\", sub_text)\n",
    "            if text != original_text:\n",
    "                changed = True\n",
    "            query_map[name] = text\n",
    "\n",
    "    # print(f\"query_map_replaced = {query_map}\")\n",
    "\n",
    "    # Step 3: Assemble all section_main and join entries in order\n",
    "    df_main = df.filter(df[\"type\"].isin([\"section_main\", \"join\"]))\n",
    "    df_main = df_main.withColumn(\"section_id\", df_main[\"section_id\"].cast(\"int\"))\n",
    "    ordered_rows = df_main.sort(\"section_id\").collect()\n",
    "\n",
    "    # print(f\"ordered_rows = {ordered_rows}\")\n",
    "    \n",
    "    final_parts = []\n",
    "    for row in ordered_rows:\n",
    "        identifier = row['identifier']\n",
    "        resolved = query_map.get(identifier, row['converted_content'] or row['content'])\n",
    "        final_parts.append(resolved.strip() if isinstance(resolved, str) else \"\")\n",
    "\n",
    "    return \"\\n\\n\".join(final_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0390417a-91a7-4db9-8b75-191eb9e22005",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Append Final Query to Dataframe"
    }
   },
   "outputs": [],
   "source": [
    "def append_final_query_row(df, final_query_str):\n",
    "    \"\"\"\n",
    "    Appends a new row with identifier 'final_query' and the provided final query string\n",
    "    into the 'final_query' column. Other fields set to None or empty.\n",
    "    Assumes DataFrame has the schema with all expected columns.\n",
    "    \"\"\"\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"section\", StringType(), True),\n",
    "        StructField(\"section_id\", IntegerType(), True),\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"identifier\", StringType(), True),\n",
    "        StructField(\"is_scalar\", BooleanType(), True),\n",
    "        StructField(\"original_query\", StringType(), True),\n",
    "        StructField(\"final_query\", StringType(), True),\n",
    "        StructField(\"content\", StringType(), True),\n",
    "        StructField(\"columns\", ArrayType(StringType()), True),\n",
    "        StructField(\"tables\", ArrayType(StringType()), True),\n",
    "        StructField(\"functions\", ArrayType(StringType()), True),\n",
    "        StructField(\"converted_content\", StringType(), True),\n",
    "        StructField(\"converted_columns\", ArrayType(StringType()), True),\n",
    "        StructField(\"response_error\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    final_query_tables = extract_tables(final_query_str)\n",
    "    final_query_functions = extract_sql_functions(final_query_str)\n",
    "\n",
    "    # build single new row\n",
    "    new_row_data = [{\n",
    "        \"section\": \"final_query\",\n",
    "        \"section_id\": 0,\n",
    "        \"type\": \"final\",\n",
    "        \"identifier\": \"final_query\",\n",
    "        \"is_scalar\": False,\n",
    "        \"original_query\": None,\n",
    "        \"final_query\": final_query_str,\n",
    "        \"content\": None,\n",
    "        \"columns\": [],\n",
    "        \"tables\": final_query_tables,\n",
    "        \"tables\": final_query_functions,\n",
    "        \"converted_content\": None,\n",
    "        \"converted_columns\": [],\n",
    "        \"response_error\": None\n",
    "    }]\n",
    "    new_row_df = spark.createDataFrame(new_row_data, schema)\n",
    "\n",
    "    # union with existing DataFrame\n",
    "    final_df = df.unionByName(new_row_df)\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6808fbde-43f9-4663-83a8-a918112d5ff5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Populate Tables Column"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "def populate_existing_tables_column(df, column=\"content\"):\n",
    "    \"\"\"\n",
    "    Updates the existing 'tables' column for each row using extract_tables \n",
    "    on the 'content' column.\n",
    "    \"\"\"\n",
    "    extract_tables_udf = udf(lambda query: extract_tables(query) if query else [], ArrayType(StringType()))\n",
    "\n",
    "    return df.withColumn(\"tables\", extract_tables_udf(col(column)))\n",
    "\n",
    "\n",
    "def get_unique_table_list(df):\n",
    "    \"\"\"\n",
    "    Returns a sorted list of unique tables from the 'tables' column of the DataFrame.\n",
    "    \"\"\"\n",
    "    tables_df = df.select(explode(\"tables\").alias(\"table\")).distinct()\n",
    "    unique_tables = [row[\"table\"] for row in tables_df.collect()]\n",
    "    return sorted(unique_tables)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d985dd2-8d31-4ae8-a5aa-e9f0742f0d82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_sql_functions(sql: str):\n",
    "    \"\"\"\n",
    "    Extracts unique SQL function names from a SQL string.\n",
    "    Matches patterns like FUNC_NAME(...).\n",
    "    \"\"\"\n",
    "    if not sql:\n",
    "        return []\n",
    "    \n",
    "    # Regex matches function calls like NVL(col, 0), UPPER(name), COUNT(*)\n",
    "    pattern = r\"\\b([A-Z_][A-Z0-9_]*)\\s*\\(\"\n",
    "    matches = re.findall(pattern, sql.upper())\n",
    "    \n",
    "    # Filter out common SQL keywords that are not functions\n",
    "    keywords = {\n",
    "        \"SELECT\", \"FROM\", \"JOIN\", \"WHERE\", \"CASE\", \"WHEN\", \"THEN\", \"ELSE\",\n",
    "        \"END\", \"GROUP\", \"ORDER\", \"HAVING\", \"ON\", \"UNION\", \"EXISTS\", \"IN\",\n",
    "        \"AS\", \"WITH\"\n",
    "    }\n",
    "    \n",
    "    return sorted(set(fn for fn in matches if fn not in keywords))\n",
    "\n",
    "def populate_existing_functions_columns(df, content_column=\"content\", columns_column=\"columns\"):\n",
    "    \"\"\"\n",
    "    Updates the DataFrame with a 'functions' column containing unique SQL functions\n",
    "    found in both the 'content' column and the 'columns' array column.\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import udf, col, array_union, flatten\n",
    "    from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "    def extract_functions_from_row(content, columns):\n",
    "        funcs = set(extract_sql_functions(content) if content else [])\n",
    "        if columns:\n",
    "            for col_expr in columns:\n",
    "                funcs.update(extract_sql_functions(col_expr) if col_expr else [])\n",
    "        return sorted(funcs)\n",
    "\n",
    "    extract_functions_udf = udf(extract_functions_from_row, ArrayType(StringType()))\n",
    "\n",
    "    return df.withColumn(\"functions\", extract_functions_udf(col(content_column), col(columns_column)))\n",
    "\n",
    "def get_unique_function_list(df):\n",
    "    \"\"\"\n",
    "    Returns a sorted list of unique functions from the 'functions' column of the DataFrame.\n",
    "    \"\"\"\n",
    "    tables_df = df.select(explode(\"functions\").alias(\"function\")).distinct()\n",
    "    unique_functions = [row[\"function\"] for row in tables_df.collect()]\n",
    "    return sorted(unique_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed1b5f90-e4d3-410c-95d3-aedd01347f3d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test Query"
    }
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# def generate_complex_query():\n",
    "#     base_query = \"\"\"\n",
    "# WITH\n",
    "# DeptStats AS (\n",
    "#     SELECT\n",
    "#         DepartmentID,\n",
    "#         SUM(Salary) AS TotalDeptSalary,\n",
    "#         COUNT(*) AS NumEmployees,\n",
    "#         AVG(Salary) AS AvgDeptSalary\n",
    "#     FROM\n",
    "#         Employees\n",
    "#     GROUP BY\n",
    "#         DepartmentID\n",
    "# ),\n",
    "# EmpProjects AS (\n",
    "#     SELECT\n",
    "#         EmployeeID,\n",
    "#         COUNT(ProjectID) AS ProjectsCompleted,\n",
    "#         MAX(CompletedDate) AS LastProjectDate,\n",
    "#         YEAR(MAX(CompletedDate)) AS LastProjectYear\n",
    "#     FROM\n",
    "#         Projects\n",
    "#     WHERE\n",
    "#         Status = 'Completed'\n",
    "#     GROUP BY\n",
    "#         EmployeeID\n",
    "# )\n",
    "\n",
    "# SELECT\n",
    "#     e.EmployeeID,\n",
    "#     UPPER(e.Name) AS Name,\n",
    "#     d.Name AS Department,\n",
    "#     CASE\n",
    "#         WHEN e.Salary > (\n",
    "#             SELECT AVG(Salary)\n",
    "#             FROM Employees\n",
    "#             WHERE DepartmentID = e.DepartmentID\n",
    "#         ) THEN CONCAT('Above Average (', CAST(e.Salary AS VARCHAR), ')')\n",
    "#         ELSE 'Average or Below'\n",
    "#     END AS SalaryStatus,\n",
    "#     -- some remark here,\n",
    "#     CASE\n",
    "#         WHEN    rsm.investment_type = 'BL'\n",
    "#             AND NVL (psah.acrd_cd, 'N') NOT IN ('Y', 'V') -- story 897300\n",
    "#         THEN\n",
    "#             NVL (\n",
    "#                 (SELECT wacoupon\n",
    "#                    FROM stg_wso_pos_acr_ame\n",
    "#                   WHERE     portfolio_fund_id = psah.cal_dt\n",
    "#                         AND asofdate = psah.cal_dt\n",
    "#                         AND asset_primaryud = psah.asset_id\n",
    "#                         AND rec_typ_cd = 'POS'),\n",
    "#                 0)\n",
    "#         ELSE\n",
    "#             psah.int_rt\n",
    "#     END\n",
    "#         AS pos_int_it,  \n",
    "#     ep.ProjectsCompleted,\n",
    "#     YEAR(e.HireDate) AS HireYear,\n",
    "#     MONTH(e.HireDate) AS HireMonth,\n",
    "#     COALESCE(ep.LastProjectYear, 'N/A') AS LastProjectYear\n",
    "# FROM\n",
    "#     Employees e\n",
    "#     JOIN Departments d ON e.DepartmentID = d.DepartmentID\n",
    "#     LEFT JOIN EmpProjects ep ON e.EmployeeID = ep.EmployeeID\n",
    "# WHERE\n",
    "#     e.EmployeeID IN (\n",
    "#         SELECT\n",
    "#             e2.EmployeeID\n",
    "#         FROM\n",
    "#             Employees e2\n",
    "#             JOIN Departments d2 ON e2.DepartmentID = d2.DepartmentID\n",
    "#             LEFT JOIN EmpProjects ep2 ON e2.EmployeeID = ep2.EmployeeID\n",
    "#         WHERE\n",
    "#             e2.Salary > (\n",
    "#                 SELECT AVG(Salary)\n",
    "#                 FROM Employees\n",
    "#                 WHERE DepartmentID = e2.DepartmentID\n",
    "#             )\n",
    "#     )\n",
    "# UNION ALL\n",
    "# SELECT\n",
    "#     NULL AS EmployeeID,\n",
    "#     NULL AS Name,\n",
    "#     d.Name AS Department,\n",
    "#     CONCAT('Department Total: ', CAST(ds.TotalDeptSalary AS VARCHAR)) AS SalaryStatus,\n",
    "#     ds.NumEmployees AS ProjectsCompleted,\n",
    "#     NULL AS HireYear,\n",
    "#     NULL AS HireMonth,\n",
    "#     NULL AS LastProjectYear,\n",
    "# \"\"\"\n",
    "\n",
    "#     # Generate 599 columns, each randomly a literal or a CASE with subquery\n",
    "#     cols = []\n",
    "#     for i in range(1, 600):\n",
    "#         rnd = random.random()\n",
    "#         if rnd < 0.05:\n",
    "#             col = (\n",
    "#                 f\"CASE WHEN ds.TotalDeptSalary > 100000 AND ds.TotalDeptSalary < 200000 AND NVL(a.emp_class, 'N') NOT IN ('A', 'B') -- story 1234\\n\"\n",
    "#                 f\"THEN NVL((SELECT id FROM Employees JOIN dept ON Employees.dept_id = dept.dept_id\\n\"\n",
    "#                 f\"WHERE DepartmentID = ds.DepartmentID),0)\\n\"\n",
    "#                 f\"ELSE 0 END as col{i}\"\n",
    "#             )\n",
    "#         elif rnd >= 0.05 and rnd < 0.1:\n",
    "#             col = (\n",
    "#                 f\"CASE WHEN ds.TotalDeptSalary > 100000 AND ds.TotalDeptSalary < 200000 AND NVL(a.emp_class, 'N') NOT IN ('A', 'B') -- story 1234\\n\"\n",
    "#                 f\"THEN NVL((SELECT id FROM Employees WHERE DepartmentID = ds.DepartmentID),2)\\n\"\n",
    "#                 f\"ELSE 0 END as col{i}\"\n",
    "#             )\n",
    "#         elif rnd >= 0.1 and rnd < 0.2:\n",
    "#             col = (\n",
    "#                 f\"CASE WHEN ds.TotalDeptSalary > 100000 AND ds.TotalDeptSalary < 200000 AND NVL(a.emp_class, 'N') NOT IN ('A', 'B') -- story 1234\\n\"\n",
    "#                 f\"THEN NVL((SELECT id FROM Employees WHERE DepartmentID = ds.DepartmentID),'x')\\n\"\n",
    "#                 f\"ELSE 0 END as col{i}\"\n",
    "#             )\n",
    "#         elif rnd >= 0.2 and rnd < 0.25:\n",
    "#             col = f\"myfunc_from({rnd}) as col{i}\"\n",
    "#         elif rnd >= 0.25 and rnd < 0.3:\n",
    "#             col = f\"myfunc_fromchar({rnd}) as col{i}\"\n",
    "#         else:\n",
    "#             col = f\"'col{i}' as col{i}\"\n",
    "#         cols.append(col)\n",
    "#     cols_str = \",\\n\".join(cols)\n",
    "\n",
    "#     # Assemble the final query\n",
    "#     final_query = f\"\"\"\n",
    "# {base_query}\n",
    "# {cols_str}\n",
    "# FROM\n",
    "#     DeptStats ds\n",
    "#     JOIN Departments d ON ds.DepartmentID = d.DepartmentID;\n",
    "# \"\"\"\n",
    "#     return final_query\n",
    "\n",
    "# query_string = generate_complex_query()\n",
    "# print(query_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "750bae44-3511-41bb-9a6a-6ba266c57809",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test Parser"
    }
   },
   "outputs": [],
   "source": [
    "# parse_log_table=\"users.paul_signorelli.sql_parsing_log\"\n",
    "# endpoint_name=\"\"\n",
    "# initialize_empty_subquery_delta_table(table_name=parse_log_table)\n",
    "# spark_df = subqueries_to_spark_dataframe(query_string)\n",
    "# spark_df_with_columns = extract_columns_and_replace_select(spark_df)\n",
    "# spark_df_with_columns_and_tables = populate_existing_tables_column(spark_df_with_columns)\n",
    "# spark_df_with_columns_and_functions = populate_existing_functions_columns(spark_df_with_columns_and_tables)\n",
    "\n",
    "# spark_df_converted = convert_sql(spark_df_with_columns_and_functions, endpoint_name=endpoint_name)\n",
    "# spark_df_with_converted_columns = convert_sql_on_columns(spark_df_converted, chunk_size=5, endpoint_name=endpoint_name)\n",
    "\n",
    "# final_query = assemble_final_query_string(spark_df_with_converted_columns)\n",
    "# pretty_final = prettify_final(final_query)\n",
    "\n",
    "# df_final = append_final_query_row(spark_df_with_converted_columns, pretty_final)\n",
    "\n",
    "# write_subqueries_to_delta(df_final, table_name=parse_log_table)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f3f42a-14e2-44bc-96a6-cc22c3406b12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# unique_table_list = get_unique_table_list(spark_df_with_columns_and_functions)\n",
    "# print(unique_table_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62a24a43-4b4d-40a4-a008-7dad910aecfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# unique_function_list = get_unique_function_list(spark_df_with_columns_and_functions)\n",
    "# print(unique_function_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f343c07e-7521-4b5e-be8a-ea10e305e0d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# UPDATE users.paul_signorelli.sql_parsing_log \n",
    "# SET converted_content = '' \n",
    "# WHERE identifier = 'subquery_select[4]_1';\n",
    "\n",
    "# UPDATE users.paul_signorelli.sql_parsing_log \n",
    "# SET converted_columns = array() \n",
    "# WHERE identifier = 'subquery_select[4]_2';\n",
    "\n",
    "# select * from users.paul_signorelli.sql_parsing_log where identifier in ('subquery_select[4]_1', 'subquery_select[4]_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adaf3147-0f72-4693-8014-f4d0728c5039",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df = load_parse_log(parse_log_table)\n",
    "\n",
    "# spark_df_converted = convert_sql(df, endpoint_name=endpoint_name, full_refresh=False)\n",
    "# spark_df_with_converted_columns = convert_sql_on_columns(spark_df_converted, chunk_size=5, endpoint_name=endpoint_name, full_refresh=False)\n",
    "\n",
    "# final_query = assemble_final_query_string(spark_df_with_converted_columns)\n",
    "# pretty_final = prettify_final(final_query)\n",
    "\n",
    "# df_final = append_final_query_row(spark_df_with_converted_columns, pretty_final)\n",
    "# write_subqueries_to_delta(df_final, table_name=parse_log_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8e56e93-df60-40e8-a209-cdbdb6312d28",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"columns\":337,\"final_query\":780},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753987373875}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Display Test Results"
    }
   },
   "outputs": [],
   "source": [
    "# display(\n",
    "#   spark.sql(f\"select * from {parse_log_table}\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14db265c-ba78-4aa7-a483-995c22edb3e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# duplicates_df = find_duplicate_subqueries_by_table_spark(df_final, \"content\")\n",
    "# display(duplicates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5d30786-fe72-4e07-8af8-a8c1936be972",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Deprecated"
    }
   },
   "outputs": [],
   "source": [
    "# def process_sections_with_subquery_extraction(sql: str):\n",
    "#     \"\"\"\n",
    "#     Runs extract_and_replace_subqueries on each 'cte' and 'select' section.\n",
    "#     Keeps 'join' sections unchanged.\n",
    "\n",
    "#     Returns a list of dicts:\n",
    "#     [\n",
    "#         {\n",
    "#             \"type\": \"cte\" | \"select\" | \"join\",\n",
    "#             \"content\": \"modified content\",\n",
    "#             \"subqueries\": [ (placeholder, original_subquery), ... ]\n",
    "#         },\n",
    "#         ...\n",
    "#     ]\n",
    "#     \"\"\"\n",
    "#     sections = split_main_sections_with_classification(sql)\n",
    "#     processed = []\n",
    "\n",
    "#     for section in sections:\n",
    "#         if section[\"type\"] in (\"cte\", \"select\"):\n",
    "#             replaced_sql, subqueries = extract_and_replace_subqueries(\n",
    "#                 section[\"content\"], \n",
    "#                 section=section[\"type\"],\n",
    "#                 section_id=section[\"id\"]\n",
    "#             )\n",
    "#             processed.append({\n",
    "#                 \"section_id\": section[\"id\"],\n",
    "#                 \"type\": section[\"type\"],\n",
    "#                 \"content\": replaced_sql,\n",
    "#                 \"subqueries\": subqueries\n",
    "#             })\n",
    "#         else:  # e.g., join, unknown\n",
    "#             processed.append({\n",
    "#                 \"section_id\": section[\"id\"],\n",
    "#                 \"type\": section[\"type\"],\n",
    "#                 \"content\": section[\"content\"],\n",
    "#                 \"subqueries\": []\n",
    "#             })\n",
    "\n",
    "#     return processed\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_c75093c8-0895-475e-8c1b-6acacfe3368b",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "sql_parser",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
