{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "105346d0-c7e4-4fab-8516-e2f4d0064e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Simple Query Parser\n",
    "\n",
    "This parser will deconstuct a complex query into its sub-queries so each of those sub-queries can be separated out to be analyzed or converted.\n",
    "\n",
    "It will look for SELECT, cte's defined by WITH, IN or EXISTS to discover sub-queries.\n",
    "\n",
    "It will also can remove the columns if need be if a query has a long list of columns and replace them with a '*'.  This can be done to reduce query size for an LLM and analyze the columns separately.\n",
    "\n",
    "The script will put all of this information into a collection which can be iterated through to call an LLM to analyze or migrate the code and then it can be pieced back together at the end.\n",
    "\n",
    "So the following query:\n",
    "\n",
    "```\n",
    "with cte1 as (\n",
    "        select cust_id, sum(sales) as sales_sum \n",
    "        from orders\n",
    "        group by cust_id\n",
    "    ),\n",
    "    cte2 as (\n",
    "        select cust_id, sum(expenses) as expenses\n",
    "        from expenses\n",
    "        group by cust_id\n",
    "    )\n",
    "    select cust_id, sales_sum, expenses\n",
    "    from cte1\n",
    "    inner join cte2 on cte1.cust_id = cte2.cust_id\n",
    "    where cte1.cust_id in (select cust_id from customer_region where region = 'USA')\n",
    "    and exists (select 1 from sales_person_region where region = 'NYC')\n",
    "```\n",
    "\n",
    "will be deconstructed to look like\n",
    "\n",
    "```\n",
    "\n",
    "Query 1 (cte1):\n",
    "    select * \n",
    "        from orders\n",
    "        group by cust_id\n",
    "Columns:\n",
    "  - cust_id\n",
    "  - sum(sales) as sales_sum\n",
    "----------------------------------------\n",
    "Query 2 (cte2):\n",
    "    select *\n",
    "        from expenses\n",
    "        group by cust_id\n",
    "Columns:\n",
    "  - cust_id\n",
    "  - sum(expenses) as expenses\n",
    "----------------------------------------\n",
    "Query 3 (subquery):\n",
    "    select * from customer_region where region = 'USA'\n",
    "Columns:\n",
    "  - cust_id\n",
    "----------------------------------------\n",
    "Query 4 (subquery):\n",
    "    select * from sales_person_region where region = 'NYC'\n",
    "Columns:\n",
    "  - 1\n",
    "----------------------------------------\n",
    "Query 5 (main):\n",
    "    select *\n",
    "    from cte1\n",
    "    inner join cte2 on cte1.cust_id = cte2.cust_id\n",
    "    where cte1.cust_id in (<<query 2>>)\n",
    "    and exists (<<query 3>>)\n",
    "Columns:\n",
    "  - cust_id\n",
    "  - sales_sum\n",
    "  - expenses\n",
    "----------------------------------------\n",
    "main query:\n",
    "with\n",
    "    cte1 as (\n",
    "        <<query 1>>\n",
    "    ),\n",
    "    cte2 as (\n",
    "        <<query 2>>\n",
    "    )\n",
    "   <<query 5>>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebea5010-eca2-4cac-b1b8-73e406ae03b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install sqlparse\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7015136a-c03f-4cce-a5ea-b9c49942ca1f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SQL Parser"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "def extract_columns(query: str) -> List[str]:\n",
    "    query = query.strip()\n",
    "    lower = query.lower()\n",
    "    start = lower.find(\"select\")\n",
    "    if start == -1:\n",
    "        return []\n",
    "\n",
    "    depth = 0\n",
    "    i = start + 6\n",
    "    cols = ''\n",
    "    while i < len(query):\n",
    "        if query[i] == '(':\n",
    "            depth += 1\n",
    "        elif query[i] == ')':\n",
    "            depth -= 1\n",
    "        elif lower[i:i+5] == ' from' and depth == 0:\n",
    "            cols = query[start + 6:i].strip()\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    columns = []\n",
    "    current = ''\n",
    "    depth = 0\n",
    "    for c in cols:\n",
    "        if c == ',' and depth == 0:\n",
    "            columns.append(current.strip())\n",
    "            current = ''\n",
    "        else:\n",
    "            current += c\n",
    "            if c == '(':\n",
    "                depth += 1\n",
    "            elif c == ')':\n",
    "                depth -= 1\n",
    "    if current.strip():\n",
    "        columns.append(current.strip())\n",
    "    return columns\n",
    "\n",
    "\n",
    "def replace_columns_with_star(query: str) -> str:\n",
    "    query = query.strip()\n",
    "    lower = query.lower()\n",
    "    start = lower.find(\"select\")\n",
    "    if start == -1:\n",
    "        return query\n",
    "\n",
    "    depth = 0\n",
    "    i = start + 6\n",
    "    while i < len(query):\n",
    "        if query[i] == '(':\n",
    "            depth += 1\n",
    "        elif query[i] == ')':\n",
    "            depth -= 1\n",
    "        elif lower[i:i+5] == ' from' and depth == 0:\n",
    "            return query[:start + 6] + ' * ' + query[i:]\n",
    "        i += 1\n",
    "    return query\n",
    "\n",
    "\n",
    "def extract_subqueries(query: str) -> Tuple[str, List[str]]:\n",
    "    subqueries = []\n",
    "    output = \"\"\n",
    "    i = 0\n",
    "    n = len(query)\n",
    "\n",
    "    while i < n:\n",
    "        if query[i] == '(':\n",
    "            start = i\n",
    "            depth = 1\n",
    "            i += 1\n",
    "            content_start = i\n",
    "            while i < n and depth > 0:\n",
    "                if query[i] == '(':\n",
    "                    depth += 1\n",
    "                elif query[i] == ')':\n",
    "                    depth -= 1\n",
    "                i += 1\n",
    "            content_end = i - 1\n",
    "            content = query[content_start:content_end].strip()\n",
    "\n",
    "            if re.search(r'\\bselect\\b', content, re.IGNORECASE):\n",
    "                temp_depth = 0\n",
    "                has_top_level_select = False\n",
    "                for m in re.finditer(r'\\bselect\\b', content, re.IGNORECASE):\n",
    "                    pos = m.start()\n",
    "                    for j in range(pos):\n",
    "                        if content[j] == '(':\n",
    "                            temp_depth += 1\n",
    "                        elif content[j] == ')':\n",
    "                            temp_depth -= 1\n",
    "                    if temp_depth == 0:\n",
    "                        has_top_level_select = True\n",
    "                        break\n",
    "                if has_top_level_select:\n",
    "                    subqueries.append(content)\n",
    "                    placeholder = f\"<<subquery_{len(subqueries)}>>\"\n",
    "                    output += f\"({placeholder})\"\n",
    "                else:\n",
    "                    output += f\"({content})\"\n",
    "            else:\n",
    "                output += f\"({content})\"\n",
    "        else:\n",
    "            output += query[i]\n",
    "            i += 1\n",
    "\n",
    "    return output, subqueries\n",
    "\n",
    "def strip_comments(sql: str) -> str:\n",
    "    return re.sub(r'/\\*.*?\\*/', '', sql, flags=re.DOTALL)\n",
    "\n",
    "def split_sql_view_full(sql: str, extract_columns: bool = False) -> Tuple[List[Dict[str, object]], str, str]:\n",
    "    sql = strip_comments(sql.strip())\n",
    "    sql = sql.strip()\n",
    "    sql = re.sub(r'\\s+', ' ', sql, flags=re.IGNORECASE)\n",
    "    mode = 'select'\n",
    "\n",
    "    if sql.lower().startswith(\"with \"):\n",
    "        mode = 'with'\n",
    "        sql_body = sql[5:].lstrip()\n",
    "    elif sql.lower().startswith(\"select \"):\n",
    "        sql_body = sql\n",
    "    else:\n",
    "        raise ValueError(\"SQL must start with WITH or SELECT\")\n",
    "\n",
    "    queries = []\n",
    "    idx = 0\n",
    "    n = len(sql_body)\n",
    "\n",
    "    cte_names = []\n",
    "\n",
    "    if mode == 'with':\n",
    "        while idx < n:\n",
    "            match = re.match(r'(\\w+)\\s+as\\s+\\(', sql_body[idx:], re.IGNORECASE)\n",
    "            if not match:\n",
    "                break\n",
    "            cte_name = match.group(1)\n",
    "            cte_names.append(cte_name)\n",
    "            start_idx = idx + match.end() - 1\n",
    "            depth = 1\n",
    "            end_idx = start_idx + 1\n",
    "            while end_idx < n and depth > 0:\n",
    "                if sql_body[end_idx] == '(':\n",
    "                    depth += 1\n",
    "                elif sql_body[end_idx] == ')':\n",
    "                    depth -= 1\n",
    "                end_idx += 1\n",
    "            cte_block = sql_body[idx:end_idx].strip().rstrip(',')\n",
    "            inner_query = re.match(rf'{cte_name}\\s+as\\s+\\((.*)\\)$', cte_block, re.IGNORECASE | re.DOTALL)\n",
    "            inner_query_text = inner_query.group(1).strip() if inner_query else cte_block\n",
    "\n",
    "            columns = extract_columns(inner_query_text) if extract_columns else []\n",
    "            starred = replace_columns_with_star(inner_query_text) if extract_columns else inner_query_text\n",
    "            queries.append({\n",
    "                'name': cte_name,\n",
    "                'type': 'cte',\n",
    "                'columns': columns,\n",
    "                'query': starred,\n",
    "            })\n",
    "\n",
    "            idx = end_idx\n",
    "            while idx < n and sql_body[idx] in \" ,\\n\\t\":\n",
    "                idx += 1\n",
    "        main_query = sql_body[idx:].strip()\n",
    "    else:\n",
    "        main_query = sql_body\n",
    "\n",
    "    rewritten_main, subqueries = extract_subqueries(main_query)\n",
    "\n",
    "    for i, subquery in enumerate(subqueries):\n",
    "        cols = extract_columns(subquery) if extract_columns else []\n",
    "        starred = replace_columns_with_star(subquery) if extract_columns else subquery\n",
    "        queries.append({\n",
    "            'name': f\"subquery_{i + 1}\",\n",
    "            'type': 'subquery',\n",
    "            'columns': cols,\n",
    "            'query': starred,\n",
    "        })\n",
    "\n",
    "    cols = extract_columns(main_query) if extract_columns else []\n",
    "    rewritten_main_starred = replace_columns_with_star(rewritten_main) if extract_columns else rewritten_main\n",
    "\n",
    "    if mode == 'with':\n",
    "        full_main_query = \"WITH \" + \", \".join(\n",
    "            [f\"{name} AS (<<{name}>>)\" for name in cte_names]\n",
    "        ) + f\" {rewritten_main_starred}\"\n",
    "    else:\n",
    "        full_main_query = rewritten_main_starred\n",
    "\n",
    "    queries.append({\n",
    "        'name': 'main',\n",
    "        'type': 'main',\n",
    "        'columns': cols,\n",
    "        'query': full_main_query,\n",
    "    })\n",
    "\n",
    "    return queries, mode, full_main_query\n",
    "\n",
    "\n",
    "def print_full_queries(sql: str):\n",
    "    parsed, mode, full_main_query = split_sql_view_full(sql)\n",
    "\n",
    "    for i, entry in enumerate(parsed):\n",
    "        label = f\"{entry['name']}\"\n",
    "        print(f\"Query {i + 1} ({label}):\")\n",
    "        print(f\"    {entry['query']}\")\n",
    "        print(\"Columns:\")\n",
    "        for col in entry['columns']:\n",
    "            print(f\"  - {col}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    print(\"main query:\")\n",
    "    if mode == 'with':\n",
    "        cte_snippets = [\n",
    "            f\"    {q['name']} as (\\n        <<{q['name']}>>\\n    )\"\n",
    "            for q in parsed if q['type'] == 'cte'\n",
    "        ]\n",
    "        print(\"with\\n\" + \",\\n\".join(cte_snippets))\n",
    "        print(\"   <<main>>\")\n",
    "    else:\n",
    "        print(\"   <<main>>\")\n",
    "\n",
    "    print(f\"\\n\\nFull main query:\\n{full_main_query}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0151c995-5a95-412b-bd2b-3fb181780a9b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LLM Conversion Functions"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import ChatMessage, ChatMessageRole\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def convert_query_to_databricks_sql(query: str, endpoint_name: str = \"databricks-claude-sonnet-4\"):\n",
    "    w = WorkspaceClient()  # Initialize without timeout parameter, set timeout if supported later\n",
    "    response = w.serving_endpoints.query(\n",
    "        # name=\"databricks-claude-3-7-sonnet\",\n",
    "        name=\"databricks-claude-sonnet-4\",\n",
    "        # name=\"llama-70b-code-converstion\",\n",
    "        messages=[\n",
    "            ChatMessage(\n",
    "                role=ChatMessageRole.SYSTEM, content=\"You are a helpful assistant.\"\n",
    "            ),\n",
    "            ChatMessage(\n",
    "                role=ChatMessageRole.USER, content=f\"Please covert the following Oracle SQL query to Databricks SQL. Just return the query, no other content, including ```sql. I need a complete conversion, do not skip any lines:\\n{query}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def convert_and_get_dataframe(query_string, endpoint_name: str = \"databricks-claude-sonnet-4\"):\n",
    "    converted = []\n",
    "    parsed, mode, main_query = split_sql_view_full(query_string)\n",
    "    for i, entry in enumerate(parsed):\n",
    "        converted_query = convert_query_to_databricks_sql(entry['query'], endpoint_name)\n",
    "        converted_query = converted_query.choices[0].message.content\n",
    "        converted.append(\n",
    "            dict(\n",
    "                name=entry['name'], \n",
    "                original=entry['query'], \n",
    "                converted=converted_query\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Convert the list of dictionaries to a Spark DataFrame\n",
    "    converted_df = spark.createDataFrame(converted)\n",
    "    return converted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87265d80-08c3-4ee9-bf23-0781aa951775",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Final Query Reassembly"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "import sqlparse\n",
    "\n",
    "def assemble_final_query(converted_df):\n",
    "    # Filter out rows where name is 'main'\n",
    "    filtered_df = converted_df.filter(col(\"name\") != \"main\")\n",
    "    # Filter out rows where name is 'main'\n",
    "    filtered_df = converted_df.filter(col(\"name\") != \"main\")\n",
    "\n",
    "    # Collect the filtered rows\n",
    "    rows = filtered_df.collect()\n",
    "\n",
    "    # Loop through the rows and replace the string in the 'converted' column\n",
    "    for row in rows:\n",
    "        name_value = row['name']\n",
    "        converted_value = row['converted']\n",
    "        main_row = converted_df.filter(col(\"name\") == \"main\").first()\n",
    "        if main_row:\n",
    "            main_converted = main_row['converted']\n",
    "            updated_main_converted = main_converted.replace(f\"<<{name_value}>>\", converted_value)\n",
    "            converted_df = converted_df.withColumn(\"converted\", when(col(\"name\") == \"main\", updated_main_converted).otherwise(col(\"converted\")))\n",
    "\n",
    "    return converted_df.select(\"name\", \"original\", \"converted\")\n",
    "\n",
    "\n",
    "def prettify_final(converted_df):\n",
    "    final_query = assemble_final_query(\n",
    "        converted_df.select(\"name\", \"original\", \"converted\")\n",
    "    ).filter(\"name = 'main'\")\n",
    "    value = final_query.select(\"converted\").collect()[0][0]\n",
    "    prettified_value = sqlparse.format(value, reindent=True, keyword_case='upper')\n",
    "    prettified_value = prettified_value.replace(\"\\n\", \"\\n\")\n",
    "    return prettified_value"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "parse_sql",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
