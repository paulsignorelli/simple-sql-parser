{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "105346d0-c7e4-4fab-8516-e2f4d0064e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Simple Query Parser\n",
    "\n",
    "This parser will deconstuct a complex query into its sub-queries so each of those sub-queries can be separated out to be analyzed or converted.\n",
    "\n",
    "It will look for SELECT, cte's defined by WITH, IN or EXISTS to discover sub-queries.\n",
    "\n",
    "It will also can remove the columns if need be if a query has a long list of columns and replace them with a '*'.  This can be done to reduce query size for an LLM and analyze the columns separately.\n",
    "\n",
    "The script will put all of this information into a collection which can be iterated through to call an LLM to analyze or migrate the code and then it can be pieced back together at the end.\n",
    "\n",
    "So the following query:\n",
    "\n",
    "```\n",
    "with cte1 as (\n",
    "        select cust_id, sum(sales) as sales_sum \n",
    "        from orders\n",
    "        group by cust_id\n",
    "    ),\n",
    "    cte2 as (\n",
    "        select cust_id, sum(expenses) as expenses\n",
    "        from expenses\n",
    "        group by cust_id\n",
    "    )\n",
    "    select cust_id, sales_sum, expenses\n",
    "    from cte1\n",
    "    inner join cte2 on cte1.cust_id = cte2.cust_id\n",
    "    where cte1.cust_id in (select cust_id from customer_region where region = 'USA')\n",
    "    and exists (select 1 from sales_person_region where region = 'NYC')\n",
    "```\n",
    "\n",
    "will be deconstructed to look like\n",
    "\n",
    "```\n",
    "\n",
    "Query 1 (cte1):\n",
    "    select * \n",
    "        from orders\n",
    "        group by cust_id\n",
    "Columns:\n",
    "  - cust_id\n",
    "  - sum(sales) as sales_sum\n",
    "----------------------------------------\n",
    "Query 2 (cte2):\n",
    "    select *\n",
    "        from expenses\n",
    "        group by cust_id\n",
    "Columns:\n",
    "  - cust_id\n",
    "  - sum(expenses) as expenses\n",
    "----------------------------------------\n",
    "Query 3 (subquery):\n",
    "    select * from customer_region where region = 'USA'\n",
    "Columns:\n",
    "  - cust_id\n",
    "----------------------------------------\n",
    "Query 4 (subquery):\n",
    "    select * from sales_person_region where region = 'NYC'\n",
    "Columns:\n",
    "  - 1\n",
    "----------------------------------------\n",
    "Query 5 (main):\n",
    "    select *\n",
    "    from cte1\n",
    "    inner join cte2 on cte1.cust_id = cte2.cust_id\n",
    "    where cte1.cust_id in (<<query 2>>)\n",
    "    and exists (<<query 3>>)\n",
    "Columns:\n",
    "  - cust_id\n",
    "  - sales_sum\n",
    "  - expenses\n",
    "----------------------------------------\n",
    "main query:\n",
    "with\n",
    "    cte1 as (\n",
    "        <<query 1>>\n",
    "    ),\n",
    "    cte2 as (\n",
    "        <<query 2>>\n",
    "    )\n",
    "   <<query 5>>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebea5010-eca2-4cac-b1b8-73e406ae03b9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Libraries"
    }
   },
   "outputs": [],
   "source": [
    "%pip install sqlparse\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7015136a-c03f-4cce-a5ea-b9c49942ca1f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SQL Parser"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import re\n",
    "\n",
    "# def extract_columns(query: str) -> List[str]:\n",
    "#     query = strip_comments(query)\n",
    "#     select_match = re.search(r\"select (.*?) from\", query, re.IGNORECASE | re.DOTALL)\n",
    "#     if not select_match:\n",
    "#         return []\n",
    "\n",
    "#     col_string = select_match.group(1)\n",
    "#     columns = []\n",
    "#     current_col = \"\"\n",
    "#     paren_depth = 0\n",
    "\n",
    "#     for char in col_string:\n",
    "#         if char == '(':\n",
    "#             paren_depth += 1\n",
    "#         elif char == ')':\n",
    "#             paren_depth -= 1\n",
    "#         elif char == ',' and paren_depth == 0:\n",
    "#             columns.append(current_col.strip())\n",
    "#             current_col = \"\"\n",
    "#             continue\n",
    "#         current_col += char\n",
    "\n",
    "#     if current_col.strip():\n",
    "#         columns.append(current_col.strip())\n",
    "\n",
    "#     return columns\n",
    "\n",
    "def extract_columns(query: str) -> List[str]:\n",
    "    query = strip_comments(query).strip()\n",
    "    lower_query = query.lower()\n",
    "    select_idx = lower_query.find(\"select\")\n",
    "    if select_idx == -1:\n",
    "        return []\n",
    "\n",
    "    # start after 'select'\n",
    "    i = select_idx + len(\"select\")\n",
    "    columns_part = \"\"\n",
    "    depth = 0\n",
    "    in_single_quote = False\n",
    "    in_double_quote = False\n",
    "\n",
    "    while i < len(query):\n",
    "        c = query[i]\n",
    "        # Handle string literals\n",
    "        if c == \"'\" and not in_double_quote:\n",
    "            if in_single_quote and i + 1 < len(query) and query[i+1] == \"'\":\n",
    "                columns_part += \"''\"\n",
    "                i += 1\n",
    "            else:\n",
    "                in_single_quote = not in_single_quote\n",
    "                columns_part += c\n",
    "        elif c == '\"' and not in_single_quote:\n",
    "            in_double_quote = not in_double_quote\n",
    "            columns_part += c\n",
    "        elif in_single_quote or in_double_quote:\n",
    "            columns_part += c\n",
    "        elif c == '(':\n",
    "            depth += 1\n",
    "            columns_part += c\n",
    "        elif c == ')':\n",
    "            depth -= 1\n",
    "            columns_part += c\n",
    "        elif lower_query[i:i+4] == \"from\" and depth == 0 and not in_single_quote and not in_double_quote:\n",
    "            break  # found top-level FROM\n",
    "        else:\n",
    "            columns_part += c\n",
    "        i += 1\n",
    "\n",
    "    # Now we have the full select list\n",
    "    # Split on commas at top-level\n",
    "    columns = []\n",
    "    current = ''\n",
    "    depth = 0\n",
    "    in_single_quote = False\n",
    "    in_double_quote = False\n",
    "    for c in columns_part:\n",
    "        if c == \"'\" and not in_double_quote:\n",
    "            in_single_quote = not in_single_quote\n",
    "            current += c\n",
    "        elif c == '\"' and not in_single_quote:\n",
    "            in_double_quote = not in_double_quote\n",
    "            current += c\n",
    "        elif in_single_quote or in_double_quote:\n",
    "            current += c\n",
    "        elif c == '(':\n",
    "            depth += 1\n",
    "            current += c\n",
    "        elif c == ')':\n",
    "            depth -= 1\n",
    "            current += c\n",
    "        elif c == ',' and depth == 0:\n",
    "            columns.append(current.strip())\n",
    "            current = ''\n",
    "        else:\n",
    "            current += c\n",
    "    if current.strip():\n",
    "        columns.append(current.strip())\n",
    "\n",
    "    return columns\n",
    "\n",
    "def strip_comments(sql: str) -> str:\n",
    "    # Remove multiline comments like /* ... */\n",
    "    sql = re.sub(r'/\\*.*?\\*/', '', sql, flags=re.DOTALL)\n",
    "\n",
    "    # Remove inline and full-line comments starting with --\n",
    "    sql = re.sub(r'--[^\\n\\r]*', '', sql)\n",
    "\n",
    "    return sql\n",
    "\n",
    "def replace_columns_with_star(query: str) -> str:\n",
    "    query = strip_comments(query)\n",
    "    query = query.strip()\n",
    "    lower = query.lower()\n",
    "    start = lower.find(\"select\")\n",
    "    if start == -1:\n",
    "        return query\n",
    "\n",
    "    depth = 0\n",
    "    i = start + 6\n",
    "    while i < len(query):\n",
    "        if query[i] == '(':\n",
    "            depth += 1\n",
    "        elif query[i] == ')':\n",
    "            depth -= 1\n",
    "        elif lower[i:i+5] == ' from' and depth == 0:\n",
    "            return query[:start + 6] + ' * ' + query[i:]\n",
    "        i += 1\n",
    "    return query\n",
    "\n",
    "# def extract_subqueries(query: str) -> Tuple[str, List[str]]:\n",
    "#     subqueries = []\n",
    "\n",
    "#     def replace_subqueries(q: str) -> str:\n",
    "#         output = \"\"\n",
    "#         i = 0\n",
    "#         n = len(q)\n",
    "\n",
    "#         while i < n:\n",
    "#             if q[i] == '(':\n",
    "#                 start = i\n",
    "#                 depth = 1\n",
    "#                 i += 1\n",
    "#                 content_start = i\n",
    "#                 while i < n and depth > 0:\n",
    "#                     if q[i] == '(':\n",
    "#                         depth += 1\n",
    "#                     elif q[i] == ')':\n",
    "#                         depth -= 1\n",
    "#                     i += 1\n",
    "#                 content_end = i - 1\n",
    "#                 content = q[content_start:content_end].strip()\n",
    "\n",
    "#                 # Recursively handle inner content\n",
    "#                 rewritten, inner_subs = extract_subqueries(content)\n",
    "#                 subqueries.extend(inner_subs)\n",
    "\n",
    "#                 # if re.match(r'^\\s*select\\b', content, re.IGNORECASE):\n",
    "#                 if 'select' in content.lower():\n",
    "#                     subqueries.append(content)\n",
    "#                     placeholder = f\"<<subquery_{len(subqueries)}>>\"\n",
    "#                     output += f\"({placeholder})\"\n",
    "#                 else:\n",
    "#                     output += f\"({rewritten})\"\n",
    "#             else:\n",
    "#                 output += q[i]\n",
    "#                 i += 1\n",
    "#         return output\n",
    "\n",
    "#     # Main first recursion on parentheses\n",
    "#     rewritten_query = query\n",
    "#     prev_query = \"\"\n",
    "#     while rewritten_query != prev_query:\n",
    "#         prev_query = rewritten_query\n",
    "#         rewritten_query = replace_subqueries(rewritten_query)\n",
    "\n",
    "#     # Now also check for top-level UNION etc\n",
    "#     union_pattern = re.compile(r'(select .*?)(?=(union|intersect|except|\\Z))', re.IGNORECASE | re.DOTALL)\n",
    "#     union_matches = union_pattern.findall(rewritten_query)\n",
    "#     if len(union_matches) > 1:\n",
    "#         new_output = \"\"\n",
    "#         last_end = 0\n",
    "#         for i, (select_block, _) in enumerate(union_matches):\n",
    "#             subqueries.append(select_block.strip())\n",
    "#             placeholder = f\"<<subquery_{len(subqueries)}>>\"\n",
    "#             start_idx = rewritten_query.find(select_block, last_end)\n",
    "#             end_idx = start_idx + len(select_block)\n",
    "#             # Always insert a trailing space to prevent collisions like <<subquery>>UNION\n",
    "#             new_output += rewritten_query[last_end:start_idx] + placeholder + \" \"\n",
    "#             last_end = end_idx\n",
    "#         new_output += rewritten_query[last_end:]\n",
    "#         rewritten_query = new_output\n",
    "\n",
    "#     return rewritten_query, subqueries\n",
    "\n",
    "def extract_subqueries(query: str):\n",
    "    import re\n",
    "    subqueries = []\n",
    "\n",
    "    # # Fix NVL((SELECT ...), 0) -> NVL(<<subquery>>, 0)\n",
    "    # pattern = re.compile(r'(\\bNVL\\s*\\()\\s*\\(\\s*(SELECT .*?)\\s*\\)\\s*,', re.IGNORECASE | re.DOTALL)\n",
    "    # while True:\n",
    "    #     match = pattern.search(query)\n",
    "    #     if not match:\n",
    "    #         break\n",
    "    #     prefix = match.group(1)\n",
    "    #     select_content = match.group(2).strip()\n",
    "    #     subqueries.append(select_content)\n",
    "    #     placeholder = f\"{prefix}<<subquery_{len(subqueries)}>>,\"\n",
    "    #     query = query[:match.start()] + placeholder + query[match.end()-1:]\n",
    "\n",
    "    # Matches NVL((SELECT ...), ...) with flexibility on second argument\n",
    "    # pattern = re.compile(r'(\\bNVL\\s*)\\(\\s*\\(\\s*(SELECT .*?)\\s*\\)\\s*(,\\s*.*?\\))', re.IGNORECASE | re.DOTALL)\n",
    "    pattern = re.compile(\n",
    "        r'(\\bNVL\\s*)'       # NVL plus optional spaces\n",
    "        r'\\(\\s*'            # opening paren with optional spaces\n",
    "        r'\\(\\s*'            # second opening paren for SELECT\n",
    "        r'(SELECT[\\s\\S]*?)' # SELECT anything (dotall), non-greedy\n",
    "        r'\\s*\\)\\s*'         # closing inner paren\n",
    "        r'(,\\s*[\\s\\S]*?\\)+)', # comma, anything, then multiple closing parens\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    while True:\n",
    "        match = pattern.search(query)\n",
    "        if not match:\n",
    "            break\n",
    "        func_prefix = match.group(1)\n",
    "        select_content = match.group(2).strip()\n",
    "        remainder = match.group(3)\n",
    "\n",
    "        subqueries.append(select_content)\n",
    "        placeholder = f\"{func_prefix}({f'<<subquery_{len(subqueries)}>>'}{remainder}\"\n",
    "        query = query[:match.start()] + placeholder + query[match.end():]\n",
    "\n",
    "    def replace_subqueries(q: str) -> str:\n",
    "        output = \"\"\n",
    "        i = 0\n",
    "        n = len(q)\n",
    "        while i < n:\n",
    "            if q[i] == '(':\n",
    "                start = i\n",
    "                depth = 1\n",
    "                i += 1\n",
    "                content_start = i\n",
    "                while i < n and depth > 0:\n",
    "                    if q[i] == '(':\n",
    "                        depth += 1\n",
    "                    elif q[i] == ')':\n",
    "                        depth -= 1\n",
    "                    i += 1\n",
    "                content_end = i - 1\n",
    "                content = q[content_start:content_end].strip()\n",
    "\n",
    "                # Recursively handle inner content\n",
    "                rewritten, inner_subs = extract_subqueries(content)\n",
    "                subqueries.extend(inner_subs)\n",
    "\n",
    "                if re.match(r'^SELECT\\b', content, re.IGNORECASE):\n",
    "                    subqueries.append(content)\n",
    "                    placeholder = f\"<<subquery_{len(subqueries)}>>\"\n",
    "                    output += placeholder\n",
    "                else:\n",
    "                    output += f\"({rewritten})\"\n",
    "            else:\n",
    "                output += q[i]\n",
    "                i += 1\n",
    "        return output\n",
    "\n",
    "    # Main pass\n",
    "    rewritten_query = query\n",
    "    prev_query = \"\"\n",
    "    while rewritten_query != prev_query:\n",
    "        prev_query = rewritten_query\n",
    "        rewritten_query = replace_subqueries(rewritten_query)\n",
    "\n",
    "    # Handle UNION, INTERSECT, EXCEPT at top level\n",
    "    union_pattern = re.compile(r'(select .*?)(?=(union|intersect|except|\\Z))', re.IGNORECASE | re.DOTALL)\n",
    "    union_matches = union_pattern.findall(rewritten_query)\n",
    "    if len(union_matches) > 1:\n",
    "        new_output = \"\"\n",
    "        last_end = 0\n",
    "        for (select_block, _) in union_matches:\n",
    "            subqueries.append(select_block.strip())\n",
    "            placeholder = f\"<<subquery_{len(subqueries)}>>\"\n",
    "            start_idx = rewritten_query.find(select_block, last_end)\n",
    "            end_idx = start_idx + len(select_block)\n",
    "            new_output += rewritten_query[last_end:start_idx] + placeholder + \" \"\n",
    "            last_end = end_idx\n",
    "        new_output += rewritten_query[last_end:]\n",
    "        rewritten_query = new_output\n",
    "\n",
    "    return rewritten_query, subqueries\n",
    "\n",
    "    # unwrap top-level wrappers like NVL(SELECT ... , 0)\n",
    "    pattern = re.compile(r'(\\w+)\\s*\\(\\s*(select .*?)\\s*,.*?\\)', re.IGNORECASE | re.DOTALL)\n",
    "    while True:\n",
    "        match = pattern.search(query)\n",
    "        if not match:\n",
    "            break\n",
    "        inner_select = match.group(2)\n",
    "        # handle any internal subqueries\n",
    "        rewritten, inner_subs = extract_subqueries(inner_select)\n",
    "        subqueries.extend(inner_subs)\n",
    "        subqueries.append(inner_select)\n",
    "        placeholder = f\"<<subquery_{len(subqueries)}>>\"\n",
    "        query = query[:match.start()] + placeholder + query[match.end():]\n",
    "\n",
    "    # then process regular nested (SELECT...) \n",
    "    prev_query = \"\"\n",
    "    rewritten_query = query\n",
    "    while rewritten_query != prev_query:\n",
    "        prev_query = rewritten_query\n",
    "        rewritten_query = replace_subqueries(rewritten_query)\n",
    "\n",
    "    return rewritten_query, subqueries\n",
    "\n",
    "def split_sql_view_full(sql: str, extract_columns_flag: bool = False) -> Tuple[List[Dict[str, object]], str, str]:\n",
    "    sql = strip_comments(sql.strip())\n",
    "    sql = sql.strip()\n",
    "    sql = re.sub(r'\\s+', ' ', sql, flags=re.IGNORECASE)\n",
    "    mode = 'select'\n",
    "\n",
    "    if sql.lower().startswith(\"with \"):\n",
    "        mode = 'with'\n",
    "        sql_body = sql[5:].lstrip()\n",
    "    elif sql.lower().startswith(\"select \"):\n",
    "        sql_body = sql\n",
    "    else:\n",
    "        raise ValueError(\"SQL must start with WITH or SELECT\")\n",
    "\n",
    "    queries = []\n",
    "    idx = 0\n",
    "    n = len(sql_body)\n",
    "    cte_names = []\n",
    "\n",
    "    if mode == 'with':\n",
    "        while idx < n:\n",
    "            match = re.match(r'(\\w+)\\s+as\\s+\\(', sql_body[idx:], re.IGNORECASE)\n",
    "            if not match:\n",
    "                break\n",
    "            cte_name = match.group(1)\n",
    "            cte_names.append(cte_name)\n",
    "            start_idx = idx + match.end() - 1\n",
    "            depth = 1\n",
    "            end_idx = start_idx + 1\n",
    "            while end_idx < n and depth > 0:\n",
    "                if sql_body[end_idx] == '(':\n",
    "                    depth += 1\n",
    "                elif sql_body[end_idx] == ')':\n",
    "                    depth -= 1\n",
    "                end_idx += 1\n",
    "            cte_block = sql_body[idx:end_idx].strip().rstrip(',')\n",
    "            inner_query = re.match(rf'{cte_name}\\s+as\\s+\\((.*)\\)$', cte_block, re.IGNORECASE | re.DOTALL)\n",
    "            inner_query_text = inner_query.group(1).strip() if inner_query else cte_block\n",
    "\n",
    "            columns = extract_columns(inner_query_text) if extract_columns_flag else []\n",
    "            starred = replace_columns_with_star(inner_query_text) if extract_columns_flag else inner_query_text\n",
    "\n",
    "            queries.append({\n",
    "                'name': cte_name,\n",
    "                'type': 'cte',\n",
    "                'columns': columns,\n",
    "                'query': starred,\n",
    "            })\n",
    "\n",
    "            idx = end_idx\n",
    "            while idx < n and sql_body[idx] in \" ,\\n\\t\":\n",
    "                idx += 1\n",
    "        main_query = sql_body[idx:].strip()\n",
    "    else:\n",
    "        main_query = sql_body\n",
    "\n",
    "    rewritten_main, subqueries = extract_subqueries(main_query)\n",
    "\n",
    "    for i, subquery in enumerate(subqueries):\n",
    "        cols = extract_columns(subquery) if extract_columns_flag else []\n",
    "        starred = replace_columns_with_star(subquery) if extract_columns_flag else subquery\n",
    "        queries.append({\n",
    "            'name': f\"subquery_{i + 1}\",\n",
    "            'type': 'subquery',\n",
    "            'columns': cols,\n",
    "            'query': starred,\n",
    "        })\n",
    "\n",
    "    cols = extract_columns(main_query) if extract_columns_flag else []\n",
    "    rewritten_main_starred = replace_columns_with_star(rewritten_main) if extract_columns_flag else rewritten_main\n",
    "\n",
    "    if mode == 'with':\n",
    "        full_main_query = \"WITH \" + \", \".join(\n",
    "            [f\"{name} AS (<<{name}>>)\" for name in cte_names]\n",
    "        ) + f\" {rewritten_main_starred}\"\n",
    "    else:\n",
    "        full_main_query = rewritten_main_starred\n",
    "\n",
    "    queries.append({\n",
    "        'name': 'main',\n",
    "        'type': 'main',\n",
    "        'columns': cols,\n",
    "        'query': full_main_query,\n",
    "    })\n",
    "\n",
    "    return queries, mode, full_main_query\n",
    "\n",
    "def print_full_queries(sql: str, extract_columns_flag: bool = False):\n",
    "    parsed, mode, main_query = split_sql_view_full(sql, extract_columns_flag=extract_columns_flag)\n",
    "    for i, entry in enumerate(parsed, 1):\n",
    "        print(f\"Query {i} ({entry['name']}):\")\n",
    "        print(f\"    {entry['query']}\")\n",
    "        print(\"Columns:\")\n",
    "        for col in entry['columns']:\n",
    "            print(f\"  - {col}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    if mode == 'with':\n",
    "        print(\"main query:\")\n",
    "        ctes = [\n",
    "            f\"    {q['name']} as (\\n        <<query {i + 1}>>\\n    )\"\n",
    "            for i, q in enumerate(parsed[:-2]) if q['name'] != 'subquery'\n",
    "        ]\n",
    "        print(\"with\\n\" + \",\\n\".join(ctes))\n",
    "        print(f\"   <<query {len(parsed)}>>\")\n",
    "    else:\n",
    "        print(\"main query:\\n   <<query {len(parsed)}>>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0151c995-5a95-412b-bd2b-3fb181780a9b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LLM Conversion Functions"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import ChatMessage, ChatMessageRole\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def convert_query_to_databricks_sql(query: str, endpoint_name: str = \"databricks-claude-sonnet-4\"):\n",
    "    w = WorkspaceClient()  # Initialize without timeout parameter, set timeout if supported later\n",
    "    response = w.serving_endpoints.query(\n",
    "        # name=\"databricks-claude-3-7-sonnet\",\n",
    "        name=endpoint_name,\n",
    "        # name=\"llama-70b-code-converstion\",\n",
    "        messages=[\n",
    "            ChatMessage(\n",
    "                role=ChatMessageRole.SYSTEM, content=\"You are a helpful assistant.\"\n",
    "            ),\n",
    "            ChatMessage(\n",
    "                role=ChatMessageRole.USER, content=f\"Please covert the following Oracle SQL query to Databricks SQL. Just return the query, no other content, including ```sql. I need a complete conversion, do not skip any lines:\\n{query}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def get_split_sql_as_dataframe(query_string, extract_columns_flag: bool = False, endpoint_name: str = \"databricks-claude-sonnet-4\"):\n",
    "    subqueries = []\n",
    "    parsed, mode, main_query = split_sql_view_full(query_string, extract_columns_flag=extract_columns_flag)\n",
    "    for i, entry in enumerate(parsed):\n",
    "        subqueries.append(\n",
    "            dict(\n",
    "                name=entry['name'], \n",
    "                original=entry['query'],\n",
    "                columns=entry.get('columns', [])\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Convert the list of dictionaries to a Spark DataFrame\n",
    "    subquery_df = spark.createDataFrame(subqueries)\n",
    "    return subquery_df\n",
    "\n",
    "from pyspark.sql import Row\n",
    "import traceback\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "\n",
    "def chunk_columns(columns, chunk_size=25):\n",
    "    return [columns[i:i + chunk_size] for i in range(0, len(columns), chunk_size)]\n",
    "\n",
    "# def convert_and_get_dataframe(\n",
    "#     query_string, extract_columns_flag: bool = False, endpoint_name: str = \"databricks-claude-sonnet-4\",\n",
    "#     test_mode: bool = False):\n",
    "\n",
    "#     schema = StructType([\n",
    "#         StructField(\"name\", StringType(), True),\n",
    "#         StructField(\"original\", StringType(), True),\n",
    "#         StructField(\"converted\", StringType(), True),\n",
    "#         StructField(\"columns\", ArrayType(StringType()), True),\n",
    "#         StructField(\"converted_columns\", ArrayType(StringType()), True),\n",
    "#         StructField(\"response_error\", StringType(), True),\n",
    "#         StructField(\"status\", StringType(), True),\n",
    "#     ])\n",
    "\n",
    "#     converted = []\n",
    "#     parsed, mode, main_query = split_sql_view_full(query_string, extract_columns_flag=extract_columns_flag)\n",
    "    \n",
    "#     for i, entry in enumerate(parsed):\n",
    "#         try:\n",
    "#             if not test_mode:\n",
    "#                 response = convert_query_to_databricks_sql(entry['query'], endpoint_name)\n",
    "#                 converted_query = response.choices[0].message.content\n",
    "#             else:\n",
    "#                 converted_query = entry['query']\n",
    "#             status = \"success\"\n",
    "#             response_error = \"\"\n",
    "#         except Exception as e:\n",
    "#             converted_query = \"\"\n",
    "#             status = \"failed\"\n",
    "#             response_error = f\"{type(e).__name__}: {str(e)}\\n{traceback.format_exc(limit=2)}\"\n",
    "\n",
    "#         converted_columns_list = []\n",
    "#         if extract_columns_flag and entry.get('columns'):\n",
    "#             for chunk in chunk_columns(entry['columns'], 25):\n",
    "#                 try:\n",
    "#                     col_sql = \"SELECT \" + \", \".join(chunk) + \" FROM dummy_table\"\n",
    "#                     if not test_mode:\n",
    "#                         col_response = convert_query_to_databricks_sql(col_sql, endpoint_name)\n",
    "#                         converted_sql = col_response.choices[0].message.content\n",
    "#                     else:\n",
    "#                         converted_sql = col_sql\n",
    "#                     converted_cols = extract_columns(converted_sql)\n",
    "#                     converted_columns_list.extend(converted_cols)\n",
    "#                 except Exception as e:\n",
    "#                     converted_columns_list.append(\"-- failed to convert columns: \" + str(e))\n",
    "\n",
    "#         converted.append(dict(\n",
    "#             name=entry['name'],\n",
    "#             original=entry['query'],\n",
    "#             converted=converted_query,\n",
    "#             columns=entry.get('columns', []),\n",
    "#             converted_columns=converted_columns_list,\n",
    "#             response_error=response_error,\n",
    "#             status=status\n",
    "#         ))\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "from pyspark.sql import Row\n",
    "import traceback\n",
    "\n",
    "def convert_and_get_dataframe(\n",
    "    query_string,\n",
    "    extract_columns_flag: bool = False,\n",
    "    columns_chunk_size: int = 25,\n",
    "    endpoint_name: str = \"databricks-claude-sonnet-4\",\n",
    "    test_mode: bool = False,\n",
    "    target_table: str = None,\n",
    "    failed_attempts: int = 5\n",
    "):\n",
    "    schema = StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"original\", StringType(), True),\n",
    "        StructField(\"converted\", StringType(), True),\n",
    "        StructField(\"columns\", ArrayType(StringType()), True),\n",
    "        StructField(\"converted_columns\", ArrayType(StringType()), True),\n",
    "        StructField(\"response_error\", StringType(), True),\n",
    "        StructField(\"status\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "    converted = []\n",
    "    failed_count = 0\n",
    "    parsed, mode, main_query = split_sql_view_full(query_string, extract_columns_flag=extract_columns_flag)\n",
    "\n",
    "    for i, entry in enumerate(parsed):\n",
    "        try:\n",
    "            if not test_mode:\n",
    "                response = convert_query_to_databricks_sql(entry['query'], endpoint_name)\n",
    "                converted_query = response.choices[0].message.content\n",
    "            else:\n",
    "                converted_query = entry['query']\n",
    "            status = \"success\"\n",
    "            response_error = \"\"\n",
    "        except Exception as e:\n",
    "            failed_count += 1\n",
    "            converted_query = \"\"\n",
    "            status = \"failed\"\n",
    "            response_error = f\"{type(e).__name__}: {str(e)}\\n{traceback.format_exc(limit=2)}\"\n",
    "\n",
    "        converted_columns_list = []\n",
    "        if extract_columns_flag and entry.get('columns'):\n",
    "            for chunk in chunk_columns(entry['columns'], chunk_size=columns_chunk_size):\n",
    "                try:\n",
    "                    col_sql = \"SELECT \" + \", \".join(chunk) + \" FROM dummy_table\"\n",
    "                    if not test_mode:\n",
    "                        col_response = convert_query_to_databricks_sql(col_sql, endpoint_name)\n",
    "                        converted_sql = col_response.choices[0].message.content\n",
    "                    else:\n",
    "                        converted_sql = col_sql\n",
    "                    converted_cols = extract_columns(converted_sql)\n",
    "                    converted_columns_list.extend(converted_cols)\n",
    "                    status = \"success\"\n",
    "                    response_error = \"\"\n",
    "                except Exception as e:\n",
    "                    converted_columns_list.append(\"-- failed to convert columns: \" + str(e))\n",
    "                    failed_count += 1\n",
    "                    if failed_count >= failed_attempts:\n",
    "                        print(f\"Stopping early: {failed_count} failed attempts reached.\")\n",
    "                        status = \"failed\"\n",
    "                        response_error = f\"Stopping early: {failed_count} failed attempts reached. {type(e).__name__}: {str(e)}\\n{traceback.format_exc(limit=2)}\"\n",
    "\n",
    "        converted.append(dict(\n",
    "            name=entry['name'],\n",
    "            original=entry['query'],\n",
    "            converted=converted_query,\n",
    "            columns=entry.get('columns', []),\n",
    "            converted_columns=converted_columns_list,\n",
    "            response_error=response_error,\n",
    "            status=status\n",
    "        ))\n",
    "\n",
    "        converted_df = spark.createDataFrame(converted, schema=schema)\n",
    "\n",
    "        # If catalog/schema provided, write to unity catalog\n",
    "        try:\n",
    "            if target_table:\n",
    "                # print(f\"Writing results to table: {target_table}\")\n",
    "                converted_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(target_table)\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing to table: {target_table}\")\n",
    "\n",
    "        if failed_count >= failed_attempts:\n",
    "            print(f\"Stopping early: {failed_count} failed attempts reached.\")\n",
    "            break\n",
    "\n",
    "    return converted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87265d80-08c3-4ee9-bf23-0781aa951775",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Final Query Reassembly"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "import sqlparse\n",
    "\n",
    "# def assemble_final_query(converted_df):\n",
    "#     # Collect all rows\n",
    "#     rows = converted_df.collect()\n",
    "\n",
    "#     # Build a dict to store final rendered versions of each query\n",
    "#     query_map = {}\n",
    "\n",
    "#     # First pass: handle * replacement in each individual query (including subqueries)\n",
    "#     for row in rows:\n",
    "#         name = row['name']\n",
    "#         query_text = row['converted'] if row['converted'] else row['original']\n",
    "\n",
    "#         # Replace * with converted columns if available\n",
    "#         if row['columns'] and row['converted_columns']:\n",
    "#             all_converted_cols = \" \".join(row['converted_columns'])\n",
    "#             query_text = re.sub(r'(?i)(select\\s+)\\*', lambda m: m.group(1) + all_converted_cols, query_text, count=1)\n",
    "\n",
    "#         query_map[name] = query_text\n",
    "\n",
    "#     # Second pass: now plug subqueries into main query\n",
    "#     main_query_text = query_map.get('main', '')\n",
    "\n",
    "#     for name, subquery_text in query_map.items():\n",
    "#         if name != 'main':\n",
    "#             main_query_text = main_query_text.replace(f\"<<{name}>>\", subquery_text)\n",
    "\n",
    "#     # Update DataFrame with final assembled main\n",
    "#     updated_df = converted_df.withColumn(\n",
    "#         \"converted\",\n",
    "#         when(col(\"name\") == \"main\", main_query_text).otherwise(col(\"converted\"))\n",
    "#     )\n",
    "\n",
    "#     return updated_df.select(\"name\", \"original\", \"converted\", \"columns\", \"converted_columns\")\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "import re\n",
    "\n",
    "def assemble_final_query(converted_df, target_table: str = None):\n",
    "    \"\"\"\n",
    "    - Replaces SELECT * with converted columns if present.\n",
    "    - Recursively replaces <<subquery_x>> placeholders across all rows.\n",
    "    - Updates the main query with fully resolved query.\n",
    "    - If target_table provided, updates that Delta table's 'main' row.\n",
    "    \"\"\"\n",
    "\n",
    "    rows = converted_df.collect()\n",
    "    query_map = {}\n",
    "\n",
    "    # Build initial map with SELECT * expansion\n",
    "    for row in rows:\n",
    "        name = row['name']\n",
    "        query_text = row['converted'] if row['converted'] else row['original']\n",
    "\n",
    "        if row['columns'] and row['converted_columns']:\n",
    "            all_converted_cols = \", \".join(row['converted_columns'])\n",
    "            query_text = re.sub(\n",
    "                r'(?i)(select\\s+)\\*',\n",
    "                lambda m: m.group(1) + all_converted_cols,\n",
    "                query_text,\n",
    "                count=1\n",
    "            )\n",
    "\n",
    "        query_map[name] = query_text\n",
    "\n",
    "    # Recursive replacement of <<subquery_x>> placeholders everywhere\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        for name, text in query_map.items():\n",
    "            original_text = text\n",
    "            for sub_name, sub_text in query_map.items():\n",
    "                if sub_name != name:\n",
    "                    text = text.replace(f\"<<{sub_name}>>\", sub_text)\n",
    "            if text != original_text:\n",
    "                changed = True\n",
    "            query_map[name] = text\n",
    "\n",
    "    # Update the local DataFrame with final main query\n",
    "    final_main_query = query_map.get('main', '')\n",
    "    updated_df = converted_df.withColumn(\n",
    "        \"converted\",\n",
    "        when(col(\"name\") == \"main\", final_main_query).otherwise(col(\"converted\"))\n",
    "    )\n",
    "\n",
    "    # If target_table specified, try to update that table's 'main' row\n",
    "    if target_table:\n",
    "        try:\n",
    "            print(f\"Updating existing table {target_table} with final assembled main query...\")\n",
    "            existing_df = spark.table(target_table)\n",
    "            # Replace 'converted' where name == 'main'\n",
    "            new_main_df = existing_df.withColumn(\n",
    "                \"converted\",\n",
    "                when(col(\"name\") == \"main\", final_main_query).otherwise(col(\"converted\"))\n",
    "            )\n",
    "            # Overwrite table\n",
    "            new_main_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(target_table)\n",
    "            print(\"Update complete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating table {target_table}\")\n",
    "\n",
    "    return updated_df.select(\"name\", \"original\", \"converted\", \"columns\", \"converted_columns\")\n",
    "\n",
    "def get_main(converted_df):\n",
    "    final_query_df = assemble_final_query(\n",
    "        converted_df\n",
    "    ).filter(\"name = 'main'\")\n",
    "    value = final_query_df.select(\"converted\").collect()[0][0]\n",
    "    return value\n",
    "\n",
    "def prettify_final(query_string: str):\n",
    "    # final_query_df = assemble_final_query(\n",
    "    #     converted_df\n",
    "    # ).filter(\"name = 'main'\")\n",
    "    # value = final_query_df.select(\"converted\").collect()[0][0]\n",
    "\n",
    "    # Format with sqlparse (keeps <<>> for any missing)\n",
    "    prettified_value = sqlparse.format(query_string, reindent=True, keyword_case='upper')\n",
    "    return prettified_value\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "parse_sql",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
